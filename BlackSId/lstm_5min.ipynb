{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open('/home/rui/blacksid/Rui/normalized_data.plkz', 'rb') as f:\n",
    "    X_train = pickle.load(f, encoding='latin1')\n",
    "    y_train = pickle.load(f, encoding='latin1')\n",
    "    X_test = pickle.load(f, encoding='latin1')\n",
    "    y_test = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 750 Ti (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lasagne Seed for Reproducibility\n",
    "lasagne.random.set_rng(np.random.RandomState(1))\n",
    "\n",
    "# Sequence Length\n",
    "SEQ_LENGTH = 100\n",
    "\n",
    "# Number of units in the two hidden (LSTM) layers\n",
    "N_HIDDEN = 512\n",
    "\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .01\n",
    "\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "\n",
    "# How often should we check the output?\n",
    "PRINT_FREQ = 300\n",
    "\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "FEATURE_SIZE = 5\n",
    "PREDICT_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.14433157, -0.21922381,  0.8256126 , -0.82660353], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.16789198,  0.96389335,  0.39143583,  0.82193977,  0.87597954], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(p, batch_size = BATCH_SIZE, data=X_train, return_target=True):\n",
    "    x = np.zeros((batch_size,SEQ_LENGTH,FEATURE_SIZE), dtype=np.float32)\n",
    "    y = np.zeros((batch_size, PREDICT_SIZE), dtype=np.float32)\n",
    "\n",
    "    for n in range(batch_size):\n",
    "        ptr = n\n",
    "        x[n,:,:] = data[(p+n):(p+n+SEQ_LENGTH),:]\n",
    "        if(return_target):\n",
    "            y[n,:] = y_train[p+n+SEQ_LENGTH-1,:]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_data = gen_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(num_epochs=NUM_EPOCHS):\n",
    "    print(\"Building network ...\")\n",
    "   \n",
    "    # First, we build the network, starting with an input layer\n",
    "    # Recurrent layers expect input of shape\n",
    "    # (batch size, SEQ_LENGTH, num_features)\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, None, FEATURE_SIZE))\n",
    "\n",
    "    \n",
    "    # We now build the LSTM layer which takes l_in as the input layer\n",
    "    # We clip the gradients at GRAD_CLIP to prevent the problem of exploding gradients. \n",
    "\n",
    "    l_forward_1 = lasagne.layers.LSTMLayer(\n",
    "        l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "    l_forward_2 = lasagne.layers.LSTMLayer(\n",
    "        l_forward_1, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        only_return_final=True)\n",
    "\n",
    "    # The output of l_forward_2 of shape (batch_size, N_HIDDEN) is then passed through the softmax nonlinearity to \n",
    "    # create probability distribution of the prediction\n",
    "    # The output of this stage is (batch_size, vocab_size)\n",
    "    l_out = lasagne.layers.DenseLayer(l_forward_2, num_units=PREDICT_SIZE, W = lasagne.init.Normal(), nonlinearity=None)\n",
    "\n",
    "    # Theano tensor for the targets\n",
    "    target_values = T.matrix('target_output')\n",
    "    \n",
    "    # lasagne.layers.get_output produces a variable for the output of the net\n",
    "    network_output = lasagne.layers.get_output(l_out)\n",
    "\n",
    "    # The loss function is calculated as the mean of the (categorical) cross-entropy between the prediction and target.\n",
    "    cost = lasagne.objectives.squared_error(network_output,target_values).mean()\n",
    "\n",
    "    # Retrieve all parameters from the network\n",
    "    all_params = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "\n",
    "    # Compute AdaGrad updates for training\n",
    "    print(\"Computing updates ...\")\n",
    "    updates = lasagne.updates.adadelta(cost, all_params, LEARNING_RATE)\n",
    "\n",
    "    # Theano functions for training and computing cost\n",
    "    print(\"Compiling functions ...\")\n",
    "    train = theano.function([l_in.input_var, target_values], cost, updates=updates, allow_input_downcast=True)\n",
    "    compute_cost = theano.function([l_in.input_var, target_values], cost, allow_input_downcast=True)\n",
    "\n",
    "    # In order to generate text from the network, we need the probability distribution of the next character given\n",
    "    # the state of the network and the input (a seed).\n",
    "    # In order to produce the probability distribution of the prediction, we compile a function called probs. \n",
    "    \n",
    "    pred = theano.function([l_in.input_var],network_output,allow_input_downcast=True)\n",
    "\n",
    "    # The next function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    # The phrase is set using the variable generation_phrase.\n",
    "    # The optional input \"N\" is used to set the number of characters of text to predict. \n",
    "\n",
    "    \n",
    "    print(\"Training ...\")\n",
    "    p = 0\n",
    "    try:\n",
    "        for it in range(X_train.shape[0] * num_epochs // BATCH_SIZE-1):\n",
    "            \n",
    "            avg_cost = 0;\n",
    "            for _ in range(PRINT_FREQ):\n",
    "                x,y = gen_data(p)\n",
    "                #print(p)\n",
    "                p += BATCH_SIZE \n",
    "                if(p+BATCH_SIZE+SEQ_LENGTH >= X_train.shape[0]):\n",
    "                    print('Carriage Return')\n",
    "                    p = 0;\n",
    "                avg_cost += train(x, y)\n",
    "            print(\"Epoch {} average loss = {}\".format(it*1.0*PRINT_FREQ/X_train.shape[0]*BATCH_SIZE, avg_cost / PRINT_FREQ))\n",
    "            test_loss = 0\n",
    "            test_p = 0\n",
    "            for index in range(X_test.shape[0]//BATCH_SIZE-1):\n",
    "                x,_ = gen_data(test_p, data=X_test, return_target=False)\n",
    "                test_p += BATCH_SIZE\n",
    "                test_loss += compute_cost(x,y)\n",
    "            print(\"Epoch {} test loss = {}\".format(it*1.0*PRINT_FREQ/X_train.shape[0]*BATCH_SIZE, test_loss / (X_test.shape[0]//BATCH_SIZE-1)))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Training ...\n",
      "Carriage Return\n",
      "Epoch 0.0 average loss = 0.9928983535120884\n",
      "Epoch 0.0 test loss = 0.6482882614087577\n",
      "Carriage Return\n",
      "Epoch 1.0 average loss = 0.9875595457106828\n",
      "Epoch 1.0 test loss = 1.2340823989925962\n",
      "Carriage Return\n",
      "Epoch 2.0 average loss = 0.980946093176802\n",
      "Epoch 2.0 test loss = 1.2831533749898274\n",
      "Carriage Return\n",
      "Epoch 3.0 average loss = 0.9706784320871035\n",
      "Epoch 3.0 test loss = 0.6773978050308999\n",
      "Carriage Return\n",
      "Epoch 4.0 average loss = 0.9638860825200876\n",
      "Epoch 4.0 test loss = 0.6879052122433981\n",
      "Carriage Return\n",
      "Epoch 5.0 average loss = 0.9784950837741295\n",
      "Epoch 5.0 test loss = 6.583910908361878\n",
      "Carriage Return\n",
      "Epoch 6.0 average loss = 0.9534197646131118\n",
      "Epoch 6.0 test loss = 1.5420988689769397\n",
      "Carriage Return\n",
      "Epoch 7.000000000000001 average loss = 0.9462688105801741\n",
      "Epoch 7.000000000000001 test loss = 1.8764306427252413\n",
      "Carriage Return\n",
      "Epoch 8.0 average loss = 0.9377489638825257\n",
      "Epoch 8.0 test loss = 1.9264650212393866\n",
      "Carriage Return\n",
      "Epoch 9.0 average loss = 0.9152572617679834\n",
      "Epoch 9.0 test loss = 2.5542529869561243\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
