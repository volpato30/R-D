{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "from logistic_sgd import load_data\n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "f = gzip.open('/home/rui/Downloads/mnist.pkl.gz', 'rb')\n",
    "try:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "except:\n",
    "    train_set, valid_set, test_set = pickle.load(f)\n",
    "f.close()\n",
    "X_train, y_train = train_set\n",
    "y_train = np.asarray(y_train, dtype = np.int32)\n",
    "X_test, y_test = test_set\n",
    "y_test = np.asarray(y_test, dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, X, y, num_units=10):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.input_var = T.matrix('inputs')\n",
    "        self.target_var = T.ivector('targets')\n",
    "        self.l_in = InputLayer(shape=(None, X.shape[1]), input_var=self.input_var)\n",
    "        self.l_out = DenseLayer(self.l_in, num_units=num_units, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "        self.prediction = lasagne.layers.get_output(self.l_out)\n",
    "        self.loss = lasagne.objectives.categorical_crossentropy(self.prediction, self.target_var)\n",
    "        self.loss = self.loss.mean()\n",
    "        self.params = lasagne.layers.get_all_params(self.l_out, trainable=True)\n",
    "        self.updates = lasagne.updates.adadelta(self.loss, self.params, learning_rate=1)\n",
    "        \n",
    "        self.test_prediction = lasagne.layers.get_output(self.l_out, deterministic=True)\n",
    "        self.test_loss = lasagne.objectives.categorical_crossentropy(self.test_prediction,\n",
    "                                                            self.target_var)\n",
    "        self.test_loss = self.test_loss.mean()\n",
    "        self.test_acc = T.mean(T.eq(T.argmax(self.test_prediction, axis=1), self.target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "        self.train_fn = theano.function([self.input_var, self.target_var],\n",
    "                                        self.loss, updates=self.updates)\n",
    "        self.acc_fn = theano.function([self.input_var, self.target_var], \n",
    "                                      self.test_acc)\n",
    "        self.train()\n",
    "        \n",
    "    def train(self, num_epochs=50, batch_size=1000):\n",
    "        for epoch in range(num_epochs):\n",
    "            train_err = 0\n",
    "            train_batches = 0\n",
    "            for batch in iterate_minibatches(self.X, self.y, batch_size, shuffle=False):\n",
    "                inputs, targets = batch\n",
    "                train_err += self.train_fn(inputs, targets)\n",
    "                train_batches += 1\n",
    "\n",
    "    def eval_acc(self, X_test, y_test):\n",
    "        test_acc = 0\n",
    "        test_batches = 0\n",
    "        for batch in iterate_minibatches(X_test, y_test, 1000, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            test_acc += self.acc_fn(inputs, targets)\n",
    "            test_batches += 1\n",
    "        self.test_acc = test_acc/test_batches\n",
    "        print('overall acc is: {}'.format(self.test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall acc is: 0.9243000090122223\n"
     ]
    }
   ],
   "source": [
    "lr.eval_acc(X_test, y_testfirst layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "acc_list.append(lr.test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.matrix('inputs')\n",
    "target_var = T.ivector('target')\n",
    "l_in = InputLayer(shape=(None, 784), input_var=input_var)\n",
    "pre_layer = l_in\n",
    "hidden_layer_list = []\n",
    "for i in range(20):\n",
    "    h_layer = DenseLayer(pre_layer, num_units=512)\n",
    "    hidden_layer_list.append(h_layer)\n",
    "    pre_layer = h_layer\n",
    "output_list = lasagne.layers.get_output(hidden_layer_list)\n",
    "output_fn = theano.function([input_var], output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = output_fn(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n",
      "(50000, 512)\n"
     ]
    }
   ],
   "source": [
    "for x in feature_list:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_feature_list = output_fn(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall acc is: 0.9383000016212464\n",
      "overall acc is: 0.9223999977111816\n",
      "overall acc is: 0.8936000049114228\n",
      "overall acc is: 0.8646999955177307\n",
      "overall acc is: 0.8221999943256378\n",
      "overall acc is: 0.7636000037193298\n",
      "overall acc is: 0.6999000012874603\n",
      "overall acc is: 0.6241000056266784\n",
      "overall acc is: 0.5231000036001205\n",
      "overall acc is: 0.424700003862381\n",
      "overall acc is: 0.33680000007152555\n",
      "overall acc is: 0.25219999700784684\n",
      "overall acc is: 0.18709999918937684\n",
      "overall acc is: 0.14549999982118605\n",
      "overall acc is: 0.1169000007212162\n",
      "overall acc is: 0.1135000005364418\n",
      "overall acc is: 0.1135000005364418\n",
      "overall acc is: 0.1135000005364418\n",
      "overall acc is: 0.1135000005364418\n",
      "overall acc is: 0.1135000005364418\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    X = feature_list[i]\n",
    "    lr = LogisticRegression(X, y_train)\n",
    "    lr.eval_acc(test_feature_list[i], y_test)\n",
    "    acc_list.append(lr.test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
