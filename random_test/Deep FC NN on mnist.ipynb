{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with 1 hidden layer, 1024 neurons, no regularization, could achieve 98% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: TITAN X (Pascal) (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "from logistic_sgd import load_data\n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
    "from lasagne.regularization import regularize_network_params, l2, l1\n",
    "import gzip\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "f = gzip.open('/home/rui/Downloads/mnist.pkl.gz', 'rb')\n",
    "try:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "except:\n",
    "    train_set, valid_set, test_set = pickle.load(f)\n",
    "f.close()\n",
    "X_train, y_train = train_set\n",
    "y_train = np.asarray(y_train, dtype = np.int32)\n",
    "X_test, y_test = test_set\n",
    "y_test = np.asarray(y_test, dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with 2 fully connected hidden layers, each layer has 1024 hidden units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(input_var, hidden_neurons=1024, layers=8):\n",
    "    l_in = InputLayer(shape=(None, 784), input_var=input_var)\n",
    "    l_hidden = DenseLayer(l_in, num_units=hidden_neurons, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    for i in range(layers):\n",
    "        l_hidden = DenseLayer(l_hidden, num_units=hidden_neurons, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    l_out = DenseLayer(lasagne.layers.DropoutLayer(l_hidden), num_units=10, nonlinearity=lasagne.nonlinearities.softmax, W=lasagne.init.HeNormal())\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with adagrad algorithm\n",
    "\n",
    "## final accuracy: 97.77%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 500 took 2.511s\n",
      "  training loss:\t\t8.600414\n",
      "Epoch 2 of 500 took 2.564s\n",
      "  training loss:\t\t0.935859\n",
      "Epoch 3 of 500 took 2.384s\n",
      "  training loss:\t\t0.228391\n",
      "Epoch 4 of 500 took 2.377s\n",
      "  training loss:\t\t0.123860\n",
      "Epoch 5 of 500 took 2.394s\n",
      "  training loss:\t\t0.079580\n",
      "Epoch 6 of 500 took 2.381s\n",
      "  training loss:\t\t0.056763\n",
      "Epoch 7 of 500 took 2.383s\n",
      "  training loss:\t\t0.041604\n",
      "Epoch 8 of 500 took 2.387s\n",
      "  training loss:\t\t0.024233\n",
      "Epoch 9 of 500 took 2.460s\n",
      "  training loss:\t\t0.015570\n",
      "Epoch 10 of 500 took 2.405s\n",
      "  training loss:\t\t0.009412\n",
      "Epoch 11 of 500 took 2.408s\n",
      "  training loss:\t\t0.006705\n",
      "Epoch 12 of 500 took 2.408s\n",
      "  training loss:\t\t0.003692\n",
      "Epoch 13 of 500 took 2.401s\n",
      "  training loss:\t\t0.003016\n",
      "Epoch 14 of 500 took 2.418s\n",
      "  training loss:\t\t0.029847\n",
      "Epoch 15 of 500 took 2.402s\n",
      "  training loss:\t\t0.011502\n",
      "Epoch 16 of 500 took 2.529s\n",
      "  training loss:\t\t0.002190\n",
      "Epoch 17 of 500 took 2.420s\n",
      "  training loss:\t\t0.001141\n",
      "Epoch 18 of 500 took 2.437s\n",
      "  training loss:\t\t0.000584\n",
      "Epoch 19 of 500 took 2.525s\n",
      "  training loss:\t\t0.000319\n",
      "Epoch 20 of 500 took 2.590s\n",
      "  training loss:\t\t0.000168\n",
      "Epoch 21 of 500 took 2.583s\n",
      "  training loss:\t\t0.000127\n",
      "Epoch 22 of 500 took 2.475s\n",
      "  training loss:\t\t0.000101\n",
      "Epoch 23 of 500 took 2.388s\n",
      "  training loss:\t\t0.000074\n",
      "Epoch 24 of 500 took 2.381s\n",
      "  training loss:\t\t0.000064\n",
      "Epoch 25 of 500 took 2.383s\n",
      "  training loss:\t\t0.000056\n",
      "Epoch 26 of 500 took 2.387s\n",
      "  training loss:\t\t0.000048\n",
      "Epoch 27 of 500 took 2.393s\n",
      "  training loss:\t\t0.000048\n",
      "Epoch 28 of 500 took 2.401s\n",
      "  training loss:\t\t0.000040\n",
      "Epoch 29 of 500 took 2.391s\n",
      "  training loss:\t\t0.000035\n",
      "Epoch 30 of 500 took 2.419s\n",
      "  training loss:\t\t0.000040\n",
      "Epoch 31 of 500 took 2.383s\n",
      "  training loss:\t\t0.000030\n",
      "Epoch 32 of 500 took 2.449s\n",
      "  training loss:\t\t0.000026\n",
      "Epoch 33 of 500 took 2.380s\n",
      "  training loss:\t\t0.000026\n",
      "Epoch 34 of 500 took 2.385s\n",
      "  training loss:\t\t0.000024\n",
      "Epoch 35 of 500 took 2.390s\n",
      "  training loss:\t\t0.000022\n",
      "Epoch 36 of 500 took 2.396s\n",
      "  training loss:\t\t0.000021\n",
      "Epoch 37 of 500 took 2.390s\n",
      "  training loss:\t\t0.000020\n",
      "Epoch 38 of 500 took 2.390s\n",
      "  training loss:\t\t0.000021\n",
      "Epoch 39 of 500 took 2.395s\n",
      "  training loss:\t\t0.000016\n",
      "Epoch 40 of 500 took 2.474s\n",
      "  training loss:\t\t0.000017\n",
      "Epoch 41 of 500 took 2.553s\n",
      "  training loss:\t\t0.000017\n",
      "Epoch 42 of 500 took 2.456s\n",
      "  training loss:\t\t0.000017\n",
      "Epoch 43 of 500 took 2.467s\n",
      "  training loss:\t\t0.000015\n",
      "Epoch 44 of 500 took 2.504s\n",
      "  training loss:\t\t0.000015\n",
      "Epoch 45 of 500 took 2.489s\n",
      "  training loss:\t\t0.000015\n",
      "Epoch 46 of 500 took 2.461s\n",
      "  training loss:\t\t0.000014\n",
      "Epoch 47 of 500 took 2.532s\n",
      "  training loss:\t\t0.000014\n",
      "Epoch 48 of 500 took 2.448s\n",
      "  training loss:\t\t0.000014\n",
      "Epoch 49 of 500 took 2.471s\n",
      "  training loss:\t\t0.000011\n",
      "Epoch 50 of 500 took 2.439s\n",
      "  training loss:\t\t0.000011\n",
      "Epoch 51 of 500 took 2.381s\n",
      "  training loss:\t\t0.000011\n",
      "Epoch 52 of 500 took 2.384s\n",
      "  training loss:\t\t0.000009\n",
      "Epoch 53 of 500 took 2.386s\n",
      "  training loss:\t\t0.000010\n",
      "Epoch 54 of 500 took 2.380s\n",
      "  training loss:\t\t0.000009\n",
      "Epoch 55 of 500 took 2.382s\n",
      "  training loss:\t\t0.000009\n",
      "Epoch 56 of 500 took 2.378s\n",
      "  training loss:\t\t0.000010\n",
      "Epoch 57 of 500 took 2.380s\n",
      "  training loss:\t\t0.000009\n",
      "Epoch 58 of 500 took 2.378s\n",
      "  training loss:\t\t0.000009\n",
      "Epoch 59 of 500 took 2.394s\n",
      "  training loss:\t\t0.000009\n",
      "Epoch 60 of 500 took 2.380s\n",
      "  training loss:\t\t0.000008\n",
      "Epoch 61 of 500 took 2.590s\n",
      "  training loss:\t\t0.000008\n",
      "Epoch 62 of 500 took 2.665s\n",
      "  training loss:\t\t0.000008\n",
      "Epoch 63 of 500 took 2.546s\n",
      "  training loss:\t\t0.000007\n",
      "Epoch 64 of 500 took 2.463s\n",
      "  training loss:\t\t0.000007\n",
      "Epoch 65 of 500 took 2.535s\n",
      "  training loss:\t\t0.000006\n",
      "Epoch 66 of 500 took 2.644s\n",
      "  training loss:\t\t0.000007\n",
      "Epoch 67 of 500 took 2.532s\n",
      "  training loss:\t\t0.000007\n",
      "Epoch 68 of 500 took 2.570s\n",
      "  training loss:\t\t0.000007\n",
      "Epoch 69 of 500 took 2.421s\n",
      "  training loss:\t\t0.000007\n",
      "Epoch 70 of 500 took 2.415s\n",
      "  training loss:\t\t0.000006\n",
      "Epoch 71 of 500 took 2.398s\n",
      "  training loss:\t\t0.000007\n",
      "Epoch 72 of 500 took 2.420s\n",
      "  training loss:\t\t0.000006\n",
      "Epoch 73 of 500 took 2.476s\n",
      "  training loss:\t\t0.000006\n",
      "Epoch 74 of 500 took 2.490s\n",
      "  training loss:\t\t0.000006\n",
      "Epoch 75 of 500 took 2.584s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 76 of 500 took 2.519s\n",
      "  training loss:\t\t0.000007\n",
      "Epoch 77 of 500 took 2.522s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 78 of 500 took 2.410s\n",
      "  training loss:\t\t0.000006\n",
      "Epoch 79 of 500 took 2.388s\n",
      "  training loss:\t\t0.000006\n",
      "Epoch 80 of 500 took 2.398s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 81 of 500 took 2.561s\n",
      "  training loss:\t\t0.000006\n",
      "Epoch 82 of 500 took 2.949s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 83 of 500 took 2.485s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 84 of 500 took 2.562s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 85 of 500 took 2.533s\n",
      "  training loss:\t\t0.000006\n",
      "Epoch 86 of 500 took 2.511s\n",
      "  training loss:\t\t0.000004\n",
      "Epoch 87 of 500 took 2.517s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 88 of 500 took 2.521s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 89 of 500 took 2.512s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 90 of 500 took 2.557s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 91 of 500 took 2.589s\n",
      "  training loss:\t\t0.000004\n",
      "Epoch 92 of 500 took 2.565s\n",
      "  training loss:\t\t0.000004\n",
      "Epoch 93 of 500 took 2.552s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 94 of 500 took 2.539s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 95 of 500 took 2.571s\n",
      "  training loss:\t\t0.000004\n",
      "Epoch 96 of 500 took 2.445s\n",
      "  training loss:\t\t0.000005\n",
      "Epoch 97 of 500 took 2.441s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 98 of 500 took 2.474s\n",
      "  training loss:\t\t0.000004\n",
      "Epoch 99 of 500 took 2.415s\n",
      "  training loss:\t\t0.000004\n",
      "Epoch 100 of 500 took 2.503s\n",
      "  training loss:\t\t0.000004\n",
      "Epoch 101 of 500 took 2.435s\n",
      "  training loss:\t\t0.000004\n",
      "Epoch 102 of 500 took 2.476s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 103 of 500 took 2.416s\n",
      "  training loss:\t\t0.000004\n",
      "Epoch 104 of 500 took 2.445s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 105 of 500 took 2.381s\n",
      "  training loss:\t\t0.000004\n",
      "Epoch 106 of 500 took 2.409s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 107 of 500 took 2.418s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 108 of 500 took 2.396s\n",
      "  training loss:\t\t0.000004\n",
      "Epoch 109 of 500 took 2.400s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 110 of 500 took 2.399s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 111 of 500 took 2.402s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 112 of 500 took 2.387s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 113 of 500 took 2.455s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 114 of 500 took 2.487s\n",
      "  training loss:\t\t0.000004\n",
      "Epoch 115 of 500 took 2.466s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 116 of 500 took 2.418s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 117 of 500 took 2.450s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 118 of 500 took 2.467s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 119 of 500 took 2.417s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 120 of 500 took 2.516s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 121 of 500 took 2.452s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 122 of 500 took 2.470s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 123 of 500 took 2.465s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 124 of 500 took 2.452s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 125 of 500 took 2.581s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 126 of 500 took 2.538s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 127 of 500 took 2.460s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 128 of 500 took 2.447s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 129 of 500 took 2.559s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 130 of 500 took 2.393s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 131 of 500 took 2.411s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 132 of 500 took 2.407s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 133 of 500 took 2.625s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 134 of 500 took 2.435s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 135 of 500 took 2.431s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 136 of 500 took 2.416s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 137 of 500 took 2.462s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 138 of 500 took 2.478s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 139 of 500 took 2.444s\n",
      "  training loss:\t\t0.000003\n",
      "Epoch 140 of 500 took 2.497s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 141 of 500 took 2.554s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 142 of 500 took 2.517s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 143 of 500 took 2.436s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 144 of 500 took 2.408s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 145 of 500 took 2.545s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 146 of 500 took 2.446s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 147 of 500 took 2.436s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 148 of 500 took 2.542s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 149 of 500 took 2.530s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 150 of 500 took 2.405s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 151 of 500 took 2.563s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 152 of 500 took 2.486s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 153 of 500 took 2.509s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 154 of 500 took 2.530s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 155 of 500 took 2.460s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 156 of 500 took 2.433s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 157 of 500 took 2.430s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 158 of 500 took 2.619s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 159 of 500 took 2.420s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 160 of 500 took 2.424s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 161 of 500 took 2.416s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 162 of 500 took 2.416s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 163 of 500 took 2.551s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 164 of 500 took 2.485s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 165 of 500 took 2.482s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 166 of 500 took 2.705s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 167 of 500 took 2.640s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 168 of 500 took 2.438s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 169 of 500 took 2.407s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 170 of 500 took 2.570s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 171 of 500 took 2.541s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 172 of 500 took 2.422s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 173 of 500 took 2.401s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 174 of 500 took 2.405s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 175 of 500 took 2.409s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 176 of 500 took 2.415s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 177 of 500 took 2.476s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 178 of 500 took 2.518s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 179 of 500 took 2.426s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 180 of 500 took 2.404s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 181 of 500 took 2.404s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 182 of 500 took 2.445s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 183 of 500 took 2.404s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 184 of 500 took 2.424s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 185 of 500 took 2.402s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 186 of 500 took 2.397s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 187 of 500 took 2.399s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 188 of 500 took 2.528s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 189 of 500 took 2.488s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 190 of 500 took 2.477s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 191 of 500 took 2.425s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 192 of 500 took 2.661s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 193 of 500 took 2.635s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 194 of 500 took 2.422s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 195 of 500 took 2.418s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 196 of 500 took 2.548s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 197 of 500 took 2.627s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 198 of 500 took 2.404s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 199 of 500 took 2.686s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 200 of 500 took 2.746s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 201 of 500 took 2.628s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 202 of 500 took 2.629s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 203 of 500 took 2.471s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 204 of 500 took 2.562s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 205 of 500 took 2.402s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 206 of 500 took 2.490s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 207 of 500 took 2.795s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 208 of 500 took 2.773s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 209 of 500 took 2.739s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 210 of 500 took 2.568s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 211 of 500 took 2.445s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 212 of 500 took 2.414s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 213 of 500 took 2.480s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 214 of 500 took 2.408s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 215 of 500 took 2.508s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 216 of 500 took 2.657s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 217 of 500 took 2.712s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 218 of 500 took 2.780s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 219 of 500 took 2.757s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 220 of 500 took 2.466s\n",
      "  training loss:\t\t0.000002\n",
      "Epoch 221 of 500 took 2.409s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 222 of 500 took 2.510s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 223 of 500 took 2.552s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 224 of 500 took 2.519s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 225 of 500 took 2.430s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 226 of 500 took 2.640s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 227 of 500 took 2.512s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 228 of 500 took 2.504s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 229 of 500 took 2.482s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 230 of 500 took 2.467s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 231 of 500 took 2.469s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 232 of 500 took 2.537s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 233 of 500 took 2.471s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 234 of 500 took 2.458s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 235 of 500 took 2.463s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 236 of 500 took 2.414s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 237 of 500 took 2.416s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 238 of 500 took 2.536s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 239 of 500 took 2.522s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 240 of 500 took 2.715s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 241 of 500 took 2.745s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 242 of 500 took 2.802s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 243 of 500 took 2.796s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 244 of 500 took 2.427s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 245 of 500 took 2.460s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 246 of 500 took 2.409s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 247 of 500 took 2.462s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 248 of 500 took 2.464s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 249 of 500 took 2.457s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 250 of 500 took 2.447s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 251 of 500 took 2.481s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 252 of 500 took 2.561s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 253 of 500 took 2.588s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 254 of 500 took 2.488s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 255 of 500 took 2.475s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 256 of 500 took 2.557s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 257 of 500 took 2.500s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 258 of 500 took 2.396s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 259 of 500 took 2.480s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 260 of 500 took 2.954s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 261 of 500 took 2.500s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 262 of 500 took 3.200s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 263 of 500 took 2.564s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 264 of 500 took 2.470s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 265 of 500 took 2.417s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 266 of 500 took 2.398s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 267 of 500 took 2.399s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 268 of 500 took 2.416s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 269 of 500 took 2.455s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 270 of 500 took 2.472s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 271 of 500 took 2.474s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 272 of 500 took 2.540s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 273 of 500 took 2.423s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 274 of 500 took 2.472s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 275 of 500 took 2.415s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 276 of 500 took 2.469s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 277 of 500 took 2.417s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 278 of 500 took 2.475s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 279 of 500 took 2.489s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 280 of 500 took 2.465s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 281 of 500 took 2.567s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 282 of 500 took 2.457s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 283 of 500 took 2.401s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 284 of 500 took 2.405s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 285 of 500 took 2.600s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 286 of 500 took 2.415s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 287 of 500 took 2.517s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 288 of 500 took 2.393s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 289 of 500 took 2.383s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 290 of 500 took 2.393s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 291 of 500 took 2.393s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 292 of 500 took 2.538s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 293 of 500 took 2.555s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 294 of 500 took 2.478s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 295 of 500 took 2.546s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 296 of 500 took 2.410s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 297 of 500 took 2.463s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 298 of 500 took 2.602s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 299 of 500 took 2.624s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 300 of 500 took 2.462s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 301 of 500 took 2.507s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 302 of 500 took 2.429s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 303 of 500 took 2.413s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 304 of 500 took 2.466s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 305 of 500 took 2.475s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 306 of 500 took 2.401s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 307 of 500 took 2.390s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 308 of 500 took 2.490s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 309 of 500 took 2.404s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 310 of 500 took 2.400s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 311 of 500 took 2.400s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 312 of 500 took 2.392s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 313 of 500 took 2.384s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 314 of 500 took 2.395s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 315 of 500 took 2.426s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 316 of 500 took 2.448s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 317 of 500 took 2.486s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 318 of 500 took 2.493s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 319 of 500 took 2.530s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 320 of 500 took 2.405s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 321 of 500 took 2.454s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 322 of 500 took 2.582s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 323 of 500 took 2.569s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 324 of 500 took 2.408s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 325 of 500 took 2.403s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 326 of 500 took 2.395s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 327 of 500 took 2.547s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 328 of 500 took 2.528s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 329 of 500 took 2.517s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 330 of 500 took 2.433s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 331 of 500 took 2.519s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 332 of 500 took 2.492s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 333 of 500 took 2.488s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 334 of 500 took 2.495s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 335 of 500 took 2.496s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 336 of 500 took 2.387s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 337 of 500 took 2.390s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 338 of 500 took 2.386s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 339 of 500 took 2.405s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 340 of 500 took 2.488s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 341 of 500 took 2.556s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 342 of 500 took 2.543s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 343 of 500 took 2.537s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 344 of 500 took 2.509s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 345 of 500 took 2.463s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 346 of 500 took 2.531s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 347 of 500 took 2.432s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 348 of 500 took 2.419s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 349 of 500 took 2.509s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 350 of 500 took 2.623s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 351 of 500 took 2.527s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 352 of 500 took 2.427s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 353 of 500 took 2.392s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 354 of 500 took 2.599s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 355 of 500 took 2.448s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 356 of 500 took 2.462s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 357 of 500 took 2.416s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 358 of 500 took 2.490s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 359 of 500 took 2.555s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 360 of 500 took 2.523s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 361 of 500 took 2.416s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 362 of 500 took 2.484s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 363 of 500 took 2.407s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 364 of 500 took 2.413s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 365 of 500 took 2.546s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 366 of 500 took 2.522s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 367 of 500 took 2.444s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 368 of 500 took 2.412s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 369 of 500 took 2.408s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 370 of 500 took 2.402s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 371 of 500 took 2.492s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 372 of 500 took 2.402s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 373 of 500 took 2.427s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 374 of 500 took 2.418s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 375 of 500 took 2.424s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 376 of 500 took 2.517s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 377 of 500 took 2.473s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 378 of 500 took 2.418s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 379 of 500 took 2.762s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 380 of 500 took 2.558s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 381 of 500 took 2.655s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 382 of 500 took 2.453s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 383 of 500 took 2.419s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 384 of 500 took 2.412s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 385 of 500 took 2.542s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 386 of 500 took 2.551s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 387 of 500 took 2.487s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 388 of 500 took 2.448s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 389 of 500 took 2.459s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 390 of 500 took 2.699s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 391 of 500 took 2.750s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 392 of 500 took 2.641s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 393 of 500 took 2.509s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 394 of 500 took 2.514s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 395 of 500 took 2.562s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 396 of 500 took 2.741s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 397 of 500 took 2.598s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 398 of 500 took 2.435s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 399 of 500 took 2.426s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 400 of 500 took 2.404s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 401 of 500 took 2.385s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 402 of 500 took 2.385s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 403 of 500 took 2.391s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 404 of 500 took 2.387s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 405 of 500 took 2.456s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 406 of 500 took 2.533s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 407 of 500 took 2.539s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 408 of 500 took 2.525s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 409 of 500 took 2.512s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 410 of 500 took 2.553s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 411 of 500 took 2.434s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 412 of 500 took 2.393s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 413 of 500 took 2.435s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 414 of 500 took 2.422s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 415 of 500 took 2.401s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 416 of 500 took 2.544s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 417 of 500 took 2.393s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 418 of 500 took 2.392s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 419 of 500 took 2.399s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 420 of 500 took 2.384s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 421 of 500 took 2.399s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 422 of 500 took 2.402s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 423 of 500 took 2.394s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 424 of 500 took 2.403s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 425 of 500 took 2.419s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 426 of 500 took 2.397s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 427 of 500 took 2.405s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 428 of 500 took 2.387s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 429 of 500 took 2.422s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 430 of 500 took 2.407s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 431 of 500 took 2.407s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 432 of 500 took 2.385s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 433 of 500 took 2.391s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 434 of 500 took 2.402s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 435 of 500 took 2.401s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 436 of 500 took 2.399s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 437 of 500 took 2.413s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 438 of 500 took 2.428s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 439 of 500 took 2.703s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 440 of 500 took 2.400s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 441 of 500 took 2.410s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 442 of 500 took 2.386s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 443 of 500 took 2.393s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 444 of 500 took 2.380s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 445 of 500 took 2.384s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 446 of 500 took 2.409s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 447 of 500 took 2.419s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 448 of 500 took 2.403s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 449 of 500 took 2.402s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 450 of 500 took 2.468s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 451 of 500 took 2.559s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 452 of 500 took 2.510s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 453 of 500 took 2.734s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 454 of 500 took 2.585s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 455 of 500 took 2.430s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 456 of 500 took 2.413s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 457 of 500 took 2.404s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 458 of 500 took 2.385s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 459 of 500 took 2.421s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 460 of 500 took 2.406s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 461 of 500 took 2.472s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 462 of 500 took 2.566s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 463 of 500 took 2.409s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 464 of 500 took 2.424s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 465 of 500 took 2.431s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 466 of 500 took 2.463s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 467 of 500 took 2.493s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 468 of 500 took 2.410s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 469 of 500 took 2.470s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 470 of 500 took 2.434s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 471 of 500 took 2.437s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 472 of 500 took 2.409s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 473 of 500 took 2.392s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 474 of 500 took 2.400s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 475 of 500 took 2.411s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 476 of 500 took 2.596s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 477 of 500 took 2.527s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 478 of 500 took 2.400s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 479 of 500 took 2.574s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 480 of 500 took 2.411s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 481 of 500 took 2.385s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 482 of 500 took 2.395s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 483 of 500 took 2.399s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 484 of 500 took 2.605s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 485 of 500 took 2.520s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 486 of 500 took 2.614s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 487 of 500 took 2.440s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 488 of 500 took 2.695s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 489 of 500 took 2.539s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 490 of 500 took 2.583s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 491 of 500 took 2.668s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 492 of 500 took 2.580s\n",
      "  training loss:\t\t0.000001\n",
      "Epoch 493 of 500 took 2.536s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 494 of 500 took 2.549s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 495 of 500 took 2.593s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 496 of 500 took 2.775s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 497 of 500 took 2.517s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 498 of 500 took 2.504s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 499 of 500 took 2.579s\n",
      "  training loss:\t\t0.000000\n",
      "Epoch 500 of 500 took 2.753s\n",
      "  training loss:\t\t0.000001\n",
      "Final results:\n",
      "  test loss:\t\t\t0.251807\n",
      "  test accuracy:\t\t97.77 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "input_var = T.matrix('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "network = build_model(input_var, 1024, 8)\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "#l2_penalty = regularize_network_params(network, l2)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.adagrad(loss, params, learning_rate=0.01)\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                    dtype=theano.config.floatX)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "        # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "final_acc = test_acc / test_batches\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    final_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9210890"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasagne.layers.count_params(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add bottle neck(128 units) between input layer and hidden1, hidden1 and hidden2. \n",
    "\n",
    "## final accuracy: 96.74%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_NIN_model(input_var, hidden_neurons=1024, bottle_neck=128, layers=8):\n",
    "    l_in = InputLayer(shape=(None, 784), input_var=input_var)\n",
    "    l_b1 = DenseLayer(l_in, num_units=bottle_neck, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    l_hidden = DenseLayer(l_b1, num_units=hidden_neurons, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    for i in range(layers):\n",
    "        l_b = DenseLayer(l_hidden, num_units=bottle_neck, W=lasagne.init.HeNormal(gain='relu'))\n",
    "        l_hidden = DenseLayer(l_b, num_units=hidden_neurons, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    l_out = DenseLayer(lasagne.layers.DropoutLayer(l_hidden), num_units=10, nonlinearity=lasagne.nonlinearities.softmax, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 500 took 1.120s\n",
      "  training loss:\t\t20.279807\n",
      "Epoch 2 of 500 took 1.078s\n",
      "  training loss:\t\t2.313354\n",
      "Epoch 3 of 500 took 1.092s\n",
      "  training loss:\t\t2.301222\n",
      "Epoch 4 of 500 took 1.065s\n",
      "  training loss:\t\t2.301220\n",
      "Epoch 5 of 500 took 1.058s\n",
      "  training loss:\t\t2.301145\n",
      "Epoch 6 of 500 took 1.062s\n",
      "  training loss:\t\t2.301182\n",
      "Epoch 7 of 500 took 1.050s\n",
      "  training loss:\t\t2.301149\n",
      "Epoch 8 of 500 took 1.055s\n",
      "  training loss:\t\t2.301151\n",
      "Epoch 9 of 500 took 1.059s\n",
      "  training loss:\t\t2.301052\n",
      "Epoch 10 of 500 took 1.085s\n",
      "  training loss:\t\t2.301137\n",
      "Epoch 11 of 500 took 1.058s\n",
      "  training loss:\t\t2.301203\n",
      "Epoch 12 of 500 took 1.050s\n",
      "  training loss:\t\t2.301084\n",
      "Epoch 13 of 500 took 1.049s\n",
      "  training loss:\t\t2.301105\n",
      "Epoch 14 of 500 took 1.061s\n",
      "  training loss:\t\t2.301051\n",
      "Epoch 15 of 500 took 1.090s\n",
      "  training loss:\t\t2.301141\n",
      "Epoch 16 of 500 took 1.116s\n",
      "  training loss:\t\t2.301084\n",
      "Epoch 17 of 500 took 1.085s\n",
      "  training loss:\t\t2.301075\n",
      "Epoch 18 of 500 took 1.107s\n",
      "  training loss:\t\t2.301039\n",
      "Epoch 19 of 500 took 1.104s\n",
      "  training loss:\t\t2.301045\n",
      "Epoch 20 of 500 took 1.057s\n",
      "  training loss:\t\t2.301080\n",
      "Epoch 21 of 500 took 1.058s\n",
      "  training loss:\t\t2.301120\n",
      "Epoch 22 of 500 took 1.054s\n",
      "  training loss:\t\t2.301136\n",
      "Epoch 23 of 500 took 1.056s\n",
      "  training loss:\t\t2.301024\n",
      "Epoch 24 of 500 took 1.106s\n",
      "  training loss:\t\t2.301090\n",
      "Epoch 25 of 500 took 1.093s\n",
      "  training loss:\t\t2.301168\n",
      "Epoch 26 of 500 took 1.060s\n",
      "  training loss:\t\t2.301077\n",
      "Epoch 27 of 500 took 1.059s\n",
      "  training loss:\t\t2.301135\n",
      "Epoch 28 of 500 took 1.053s\n",
      "  training loss:\t\t2.301102\n",
      "Epoch 29 of 500 took 1.057s\n",
      "  training loss:\t\t2.301103\n",
      "Epoch 30 of 500 took 1.062s\n",
      "  training loss:\t\t2.301101\n",
      "Epoch 31 of 500 took 1.064s\n",
      "  training loss:\t\t2.301065\n",
      "Epoch 32 of 500 took 1.057s\n",
      "  training loss:\t\t2.301068\n",
      "Epoch 33 of 500 took 1.062s\n",
      "  training loss:\t\t2.301105\n",
      "Epoch 34 of 500 took 1.069s\n",
      "  training loss:\t\t2.301186\n",
      "Epoch 35 of 500 took 1.058s\n",
      "  training loss:\t\t2.301165\n",
      "Epoch 36 of 500 took 1.054s\n",
      "  training loss:\t\t2.301140\n",
      "Epoch 37 of 500 took 1.064s\n",
      "  training loss:\t\t2.301090\n",
      "Epoch 38 of 500 took 1.056s\n",
      "  training loss:\t\t2.301111\n",
      "Epoch 39 of 500 took 1.103s\n",
      "  training loss:\t\t2.301091\n",
      "Epoch 40 of 500 took 1.105s\n",
      "  training loss:\t\t2.301118\n",
      "Epoch 41 of 500 took 1.115s\n",
      "  training loss:\t\t2.301009\n",
      "Epoch 42 of 500 took 1.103s\n",
      "  training loss:\t\t2.301078\n",
      "Epoch 43 of 500 took 1.060s\n",
      "  training loss:\t\t2.301109\n",
      "Epoch 44 of 500 took 1.058s\n",
      "  training loss:\t\t2.301099\n",
      "Epoch 45 of 500 took 1.054s\n",
      "  training loss:\t\t2.301133\n",
      "Epoch 46 of 500 took 1.059s\n",
      "  training loss:\t\t2.301116\n",
      "Epoch 47 of 500 took 1.057s\n",
      "  training loss:\t\t2.301022\n",
      "Epoch 48 of 500 took 1.061s\n",
      "  training loss:\t\t2.301103\n",
      "Epoch 49 of 500 took 1.061s\n",
      "  training loss:\t\t2.301117\n",
      "Epoch 50 of 500 took 1.056s\n",
      "  training loss:\t\t2.301165\n",
      "Epoch 51 of 500 took 1.055s\n",
      "  training loss:\t\t2.301079\n",
      "Epoch 52 of 500 took 1.058s\n",
      "  training loss:\t\t2.301082\n",
      "Epoch 53 of 500 took 1.067s\n",
      "  training loss:\t\t2.301043\n",
      "Epoch 54 of 500 took 1.058s\n",
      "  training loss:\t\t2.301108\n",
      "Epoch 55 of 500 took 1.063s\n",
      "  training loss:\t\t2.301101\n",
      "Epoch 56 of 500 took 1.064s\n",
      "  training loss:\t\t2.301048\n",
      "Epoch 57 of 500 took 1.061s\n",
      "  training loss:\t\t2.301025\n",
      "Epoch 58 of 500 took 1.054s\n",
      "  training loss:\t\t2.301048\n",
      "Epoch 59 of 500 took 1.063s\n",
      "  training loss:\t\t2.301099\n",
      "Epoch 60 of 500 took 1.070s\n",
      "  training loss:\t\t2.301083\n",
      "Epoch 61 of 500 took 1.058s\n",
      "  training loss:\t\t2.301064\n",
      "Epoch 62 of 500 took 1.065s\n",
      "  training loss:\t\t2.301069\n",
      "Epoch 63 of 500 took 1.078s\n",
      "  training loss:\t\t2.301059\n",
      "Epoch 64 of 500 took 1.095s\n",
      "  training loss:\t\t2.301053\n",
      "Epoch 65 of 500 took 1.137s\n",
      "  training loss:\t\t2.301041\n",
      "Epoch 66 of 500 took 1.213s\n",
      "  training loss:\t\t2.301091\n",
      "Epoch 67 of 500 took 1.122s\n",
      "  training loss:\t\t2.301110\n",
      "Epoch 68 of 500 took 1.135s\n",
      "  training loss:\t\t2.301079\n",
      "Epoch 69 of 500 took 1.171s\n",
      "  training loss:\t\t2.301061\n",
      "Epoch 70 of 500 took 1.084s\n",
      "  training loss:\t\t2.301092\n",
      "Epoch 71 of 500 took 1.059s\n",
      "  training loss:\t\t2.301101\n",
      "Epoch 72 of 500 took 1.059s\n",
      "  training loss:\t\t2.301062\n",
      "Epoch 73 of 500 took 1.103s\n",
      "  training loss:\t\t2.301047\n",
      "Epoch 74 of 500 took 1.057s\n",
      "  training loss:\t\t2.301135\n",
      "Epoch 75 of 500 took 1.055s\n",
      "  training loss:\t\t2.301078\n",
      "Epoch 76 of 500 took 1.053s\n",
      "  training loss:\t\t2.301015\n",
      "Epoch 77 of 500 took 1.049s\n",
      "  training loss:\t\t2.301046\n",
      "Epoch 78 of 500 took 1.050s\n",
      "  training loss:\t\t2.301084\n",
      "Epoch 79 of 500 took 1.063s\n",
      "  training loss:\t\t2.301074\n",
      "Epoch 80 of 500 took 1.147s\n",
      "  training loss:\t\t2.301113\n",
      "Epoch 81 of 500 took 1.090s\n",
      "  training loss:\t\t2.301105\n",
      "Epoch 82 of 500 took 1.083s\n",
      "  training loss:\t\t2.301094\n",
      "Epoch 83 of 500 took 1.169s\n",
      "  training loss:\t\t2.301059\n",
      "Epoch 84 of 500 took 1.099s\n",
      "  training loss:\t\t2.301058\n",
      "Epoch 85 of 500 took 1.166s\n",
      "  training loss:\t\t2.301033\n",
      "Epoch 86 of 500 took 1.105s\n",
      "  training loss:\t\t2.301038\n",
      "Epoch 87 of 500 took 1.171s\n",
      "  training loss:\t\t2.301083\n",
      "Epoch 88 of 500 took 1.160s\n",
      "  training loss:\t\t2.301030\n",
      "Epoch 89 of 500 took 1.123s\n",
      "  training loss:\t\t2.301076\n",
      "Epoch 90 of 500 took 1.144s\n",
      "  training loss:\t\t2.301022\n",
      "Epoch 91 of 500 took 1.196s\n",
      "  training loss:\t\t2.301121\n",
      "Epoch 92 of 500 took 1.131s\n",
      "  training loss:\t\t2.301001\n",
      "Epoch 93 of 500 took 1.093s\n",
      "  training loss:\t\t2.301059\n",
      "Epoch 94 of 500 took 1.113s\n",
      "  training loss:\t\t2.301061\n",
      "Epoch 95 of 500 took 1.069s\n",
      "  training loss:\t\t2.301056\n",
      "Epoch 96 of 500 took 1.063s\n",
      "  training loss:\t\t2.301091\n",
      "Epoch 97 of 500 took 1.058s\n",
      "  training loss:\t\t2.301052\n",
      "Epoch 98 of 500 took 1.059s\n",
      "  training loss:\t\t2.301060\n",
      "Epoch 99 of 500 took 1.062s\n",
      "  training loss:\t\t2.301087\n",
      "Epoch 100 of 500 took 1.059s\n",
      "  training loss:\t\t2.301050\n",
      "Epoch 101 of 500 took 1.056s\n",
      "  training loss:\t\t2.301073\n",
      "Epoch 102 of 500 took 1.053s\n",
      "  training loss:\t\t2.301030\n",
      "Epoch 103 of 500 took 1.063s\n",
      "  training loss:\t\t2.301082\n",
      "Epoch 104 of 500 took 1.055s\n",
      "  training loss:\t\t2.301049\n",
      "Epoch 105 of 500 took 1.077s\n",
      "  training loss:\t\t2.301041\n",
      "Epoch 106 of 500 took 1.120s\n",
      "  training loss:\t\t2.301063\n",
      "Epoch 107 of 500 took 1.053s\n",
      "  training loss:\t\t2.301081\n",
      "Epoch 108 of 500 took 1.060s\n",
      "  training loss:\t\t2.301026\n",
      "Epoch 109 of 500 took 1.059s\n",
      "  training loss:\t\t2.300986\n",
      "Epoch 110 of 500 took 1.055s\n",
      "  training loss:\t\t2.301052\n",
      "Epoch 111 of 500 took 1.055s\n",
      "  training loss:\t\t2.301045\n",
      "Epoch 112 of 500 took 1.059s\n",
      "  training loss:\t\t2.301060\n",
      "Epoch 113 of 500 took 1.067s\n",
      "  training loss:\t\t2.301000\n",
      "Epoch 114 of 500 took 1.065s\n",
      "  training loss:\t\t2.301032\n",
      "Epoch 115 of 500 took 1.068s\n",
      "  training loss:\t\t2.301056\n",
      "Epoch 116 of 500 took 1.078s\n",
      "  training loss:\t\t2.301101\n",
      "Epoch 117 of 500 took 1.077s\n",
      "  training loss:\t\t2.301049\n",
      "Epoch 118 of 500 took 1.080s\n",
      "  training loss:\t\t2.301023\n",
      "Epoch 119 of 500 took 1.064s\n",
      "  training loss:\t\t2.301099\n",
      "Epoch 120 of 500 took 1.053s\n",
      "  training loss:\t\t2.301035\n",
      "Epoch 121 of 500 took 1.057s\n",
      "  training loss:\t\t2.301079\n",
      "Epoch 122 of 500 took 1.065s\n",
      "  training loss:\t\t2.301051\n",
      "Epoch 123 of 500 took 1.058s\n",
      "  training loss:\t\t2.301069\n",
      "Epoch 124 of 500 took 1.056s\n",
      "  training loss:\t\t2.301043\n",
      "Epoch 125 of 500 took 1.054s\n",
      "  training loss:\t\t2.301080\n",
      "Epoch 126 of 500 took 1.113s\n",
      "  training loss:\t\t2.301040\n",
      "Epoch 127 of 500 took 1.052s\n",
      "  training loss:\t\t2.301056\n",
      "Epoch 128 of 500 took 1.112s\n",
      "  training loss:\t\t2.301067\n",
      "Epoch 129 of 500 took 1.120s\n",
      "  training loss:\t\t2.301051\n",
      "Epoch 130 of 500 took 1.114s\n",
      "  training loss:\t\t2.301068\n",
      "Epoch 131 of 500 took 1.134s\n",
      "  training loss:\t\t2.301095\n",
      "Epoch 132 of 500 took 1.122s\n",
      "  training loss:\t\t2.301041\n",
      "Epoch 133 of 500 took 1.064s\n",
      "  training loss:\t\t2.301075\n",
      "Epoch 134 of 500 took 1.443s\n",
      "  training loss:\t\t2.301091\n",
      "Epoch 135 of 500 took 1.062s\n",
      "  training loss:\t\t2.301026\n",
      "Epoch 136 of 500 took 1.121s\n",
      "  training loss:\t\t2.301074\n",
      "Epoch 137 of 500 took 1.086s\n",
      "  training loss:\t\t2.301078\n",
      "Epoch 138 of 500 took 1.062s\n",
      "  training loss:\t\t2.301039\n",
      "Epoch 139 of 500 took 1.061s\n",
      "  training loss:\t\t2.301021\n",
      "Epoch 140 of 500 took 1.058s\n",
      "  training loss:\t\t2.301054\n",
      "Epoch 141 of 500 took 1.098s\n",
      "  training loss:\t\t2.301073\n",
      "Epoch 142 of 500 took 1.086s\n",
      "  training loss:\t\t2.301042\n",
      "Epoch 143 of 500 took 1.065s\n",
      "  training loss:\t\t2.301081\n",
      "Epoch 144 of 500 took 1.061s\n",
      "  training loss:\t\t2.301040\n",
      "Epoch 145 of 500 took 1.062s\n",
      "  training loss:\t\t2.301089\n",
      "Epoch 146 of 500 took 1.063s\n",
      "  training loss:\t\t2.300993\n",
      "Epoch 147 of 500 took 1.082s\n",
      "  training loss:\t\t2.301019\n",
      "Epoch 148 of 500 took 1.068s\n",
      "  training loss:\t\t2.301096\n",
      "Epoch 149 of 500 took 1.078s\n",
      "  training loss:\t\t2.301004\n",
      "Epoch 150 of 500 took 1.070s\n",
      "  training loss:\t\t2.301122\n",
      "Epoch 151 of 500 took 1.094s\n",
      "  training loss:\t\t2.301099\n",
      "Epoch 152 of 500 took 1.060s\n",
      "  training loss:\t\t2.301059\n",
      "Epoch 153 of 500 took 1.128s\n",
      "  training loss:\t\t2.301070\n",
      "Epoch 154 of 500 took 1.148s\n",
      "  training loss:\t\t2.301062\n",
      "Epoch 155 of 500 took 1.144s\n",
      "  training loss:\t\t2.301029\n",
      "Epoch 156 of 500 took 1.160s\n",
      "  training loss:\t\t2.301088\n",
      "Epoch 157 of 500 took 1.137s\n",
      "  training loss:\t\t2.301031\n",
      "Epoch 158 of 500 took 1.069s\n",
      "  training loss:\t\t2.301024\n",
      "Epoch 159 of 500 took 1.063s\n",
      "  training loss:\t\t2.301010\n",
      "Epoch 160 of 500 took 1.067s\n",
      "  training loss:\t\t2.301089\n",
      "Epoch 161 of 500 took 1.094s\n",
      "  training loss:\t\t2.301025\n",
      "Epoch 162 of 500 took 1.067s\n",
      "  training loss:\t\t2.301048\n",
      "Epoch 163 of 500 took 1.056s\n",
      "  training loss:\t\t2.301064\n",
      "Epoch 164 of 500 took 1.060s\n",
      "  training loss:\t\t2.301013\n",
      "Epoch 165 of 500 took 1.055s\n",
      "  training loss:\t\t2.301044\n",
      "Epoch 166 of 500 took 1.051s\n",
      "  training loss:\t\t2.301076\n",
      "Epoch 167 of 500 took 1.052s\n",
      "  training loss:\t\t2.301032\n",
      "Epoch 168 of 500 took 1.054s\n",
      "  training loss:\t\t2.301097\n",
      "Epoch 169 of 500 took 1.053s\n",
      "  training loss:\t\t2.301114\n",
      "Epoch 170 of 500 took 1.053s\n",
      "  training loss:\t\t2.301076\n",
      "Epoch 171 of 500 took 1.055s\n",
      "  training loss:\t\t2.301076\n",
      "Epoch 172 of 500 took 1.116s\n",
      "  training loss:\t\t2.301062\n",
      "Epoch 173 of 500 took 1.078s\n",
      "  training loss:\t\t2.301072\n",
      "Epoch 174 of 500 took 1.081s\n",
      "  training loss:\t\t2.301012\n",
      "Epoch 175 of 500 took 1.072s\n",
      "  training loss:\t\t2.301042\n",
      "Epoch 176 of 500 took 1.337s\n",
      "  training loss:\t\t2.301049\n",
      "Epoch 177 of 500 took 1.124s\n",
      "  training loss:\t\t2.301071\n",
      "Epoch 178 of 500 took 1.060s\n",
      "  training loss:\t\t2.301028\n",
      "Epoch 179 of 500 took 1.062s\n",
      "  training loss:\t\t2.301030\n",
      "Epoch 180 of 500 took 1.112s\n",
      "  training loss:\t\t2.301035\n",
      "Epoch 181 of 500 took 1.055s\n",
      "  training loss:\t\t2.301057\n",
      "Epoch 182 of 500 took 1.056s\n",
      "  training loss:\t\t2.301030\n",
      "Epoch 183 of 500 took 1.060s\n",
      "  training loss:\t\t2.301070\n",
      "Epoch 184 of 500 took 1.056s\n",
      "  training loss:\t\t2.301049\n",
      "Epoch 185 of 500 took 1.055s\n",
      "  training loss:\t\t2.301033\n",
      "Epoch 186 of 500 took 1.082s\n",
      "  training loss:\t\t2.301024\n",
      "Epoch 187 of 500 took 1.061s\n",
      "  training loss:\t\t2.301056\n",
      "Epoch 188 of 500 took 1.054s\n",
      "  training loss:\t\t2.301076\n",
      "Epoch 189 of 500 took 1.058s\n",
      "  training loss:\t\t2.301065\n",
      "Epoch 190 of 500 took 1.087s\n",
      "  training loss:\t\t2.301055\n",
      "Epoch 191 of 500 took 1.051s\n",
      "  training loss:\t\t2.301085\n",
      "Epoch 192 of 500 took 1.058s\n",
      "  training loss:\t\t2.301065\n",
      "Epoch 193 of 500 took 1.058s\n",
      "  training loss:\t\t2.301081\n",
      "Epoch 194 of 500 took 1.051s\n",
      "  training loss:\t\t2.301047\n",
      "Epoch 195 of 500 took 1.056s\n",
      "  training loss:\t\t2.301031\n",
      "Epoch 196 of 500 took 1.063s\n",
      "  training loss:\t\t2.301075\n",
      "Epoch 197 of 500 took 1.059s\n",
      "  training loss:\t\t2.301054\n",
      "Epoch 198 of 500 took 1.068s\n",
      "  training loss:\t\t2.301026\n",
      "Epoch 199 of 500 took 1.052s\n",
      "  training loss:\t\t2.301012\n",
      "Epoch 200 of 500 took 1.065s\n",
      "  training loss:\t\t2.301029\n",
      "Epoch 201 of 500 took 1.062s\n",
      "  training loss:\t\t2.301038\n",
      "Epoch 202 of 500 took 1.077s\n",
      "  training loss:\t\t2.301063\n",
      "Epoch 203 of 500 took 1.073s\n",
      "  training loss:\t\t2.301037\n",
      "Epoch 204 of 500 took 1.058s\n",
      "  training loss:\t\t2.301080\n",
      "Epoch 205 of 500 took 1.096s\n",
      "  training loss:\t\t2.301043\n",
      "Epoch 206 of 500 took 1.102s\n",
      "  training loss:\t\t2.301036\n",
      "Epoch 207 of 500 took 1.058s\n",
      "  training loss:\t\t2.301009\n",
      "Epoch 208 of 500 took 1.121s\n",
      "  training loss:\t\t2.301031\n",
      "Epoch 209 of 500 took 1.078s\n",
      "  training loss:\t\t2.301072\n",
      "Epoch 210 of 500 took 1.059s\n",
      "  training loss:\t\t2.301053\n",
      "Epoch 211 of 500 took 1.059s\n",
      "  training loss:\t\t2.301045\n",
      "Epoch 212 of 500 took 1.070s\n",
      "  training loss:\t\t2.301031\n",
      "Epoch 213 of 500 took 1.120s\n",
      "  training loss:\t\t2.301063\n",
      "Epoch 214 of 500 took 1.098s\n",
      "  training loss:\t\t2.301033\n",
      "Epoch 215 of 500 took 1.058s\n",
      "  training loss:\t\t2.301033\n",
      "Epoch 216 of 500 took 1.078s\n",
      "  training loss:\t\t2.301070\n",
      "Epoch 217 of 500 took 1.062s\n",
      "  training loss:\t\t2.300995\n",
      "Epoch 218 of 500 took 1.058s\n",
      "  training loss:\t\t2.301093\n",
      "Epoch 219 of 500 took 1.062s\n",
      "  training loss:\t\t2.301058\n",
      "Epoch 220 of 500 took 1.062s\n",
      "  training loss:\t\t2.301019\n",
      "Epoch 221 of 500 took 1.062s\n",
      "  training loss:\t\t2.301038\n",
      "Epoch 222 of 500 took 1.055s\n",
      "  training loss:\t\t2.301042\n",
      "Epoch 223 of 500 took 1.053s\n",
      "  training loss:\t\t2.301012\n",
      "Epoch 224 of 500 took 1.059s\n",
      "  training loss:\t\t2.301024\n",
      "Epoch 225 of 500 took 1.080s\n",
      "  training loss:\t\t2.301024\n",
      "Epoch 226 of 500 took 1.056s\n",
      "  training loss:\t\t2.300988\n",
      "Epoch 227 of 500 took 1.084s\n",
      "  training loss:\t\t2.301076\n",
      "Epoch 228 of 500 took 1.104s\n",
      "  training loss:\t\t2.301052\n",
      "Epoch 229 of 500 took 1.077s\n",
      "  training loss:\t\t2.301039\n",
      "Epoch 230 of 500 took 1.058s\n",
      "  training loss:\t\t2.301073\n",
      "Epoch 231 of 500 took 1.050s\n",
      "  training loss:\t\t2.301052\n",
      "Epoch 232 of 500 took 1.057s\n",
      "  training loss:\t\t2.301023\n",
      "Epoch 233 of 500 took 1.061s\n",
      "  training loss:\t\t2.301039\n",
      "Epoch 234 of 500 took 1.066s\n",
      "  training loss:\t\t2.301059\n",
      "Epoch 235 of 500 took 1.057s\n",
      "  training loss:\t\t2.301042\n",
      "Epoch 236 of 500 took 1.053s\n",
      "  training loss:\t\t2.301039\n",
      "Epoch 237 of 500 took 1.052s\n",
      "  training loss:\t\t2.301087\n",
      "Epoch 238 of 500 took 1.052s\n",
      "  training loss:\t\t2.300999\n",
      "Epoch 239 of 500 took 1.057s\n",
      "  training loss:\t\t2.301002\n",
      "Epoch 240 of 500 took 1.055s\n",
      "  training loss:\t\t2.301040\n",
      "Epoch 241 of 500 took 1.059s\n",
      "  training loss:\t\t2.301034\n",
      "Epoch 242 of 500 took 1.054s\n",
      "  training loss:\t\t2.301035\n",
      "Epoch 243 of 500 took 1.062s\n",
      "  training loss:\t\t2.301061\n",
      "Epoch 244 of 500 took 1.089s\n",
      "  training loss:\t\t2.301038\n",
      "Epoch 245 of 500 took 1.060s\n",
      "  training loss:\t\t2.301052\n",
      "Epoch 246 of 500 took 1.058s\n",
      "  training loss:\t\t2.301040\n",
      "Epoch 247 of 500 took 1.064s\n",
      "  training loss:\t\t2.301008\n",
      "Epoch 248 of 500 took 1.074s\n",
      "  training loss:\t\t2.301008\n",
      "Epoch 249 of 500 took 1.057s\n",
      "  training loss:\t\t2.301022\n",
      "Epoch 250 of 500 took 1.077s\n",
      "  training loss:\t\t2.301020\n",
      "Epoch 251 of 500 took 1.108s\n",
      "  training loss:\t\t2.301025\n",
      "Epoch 252 of 500 took 1.060s\n",
      "  training loss:\t\t2.301068\n",
      "Epoch 253 of 500 took 1.070s\n",
      "  training loss:\t\t2.301025\n",
      "Epoch 254 of 500 took 1.070s\n",
      "  training loss:\t\t2.300992\n",
      "Epoch 255 of 500 took 1.056s\n",
      "  training loss:\t\t2.301014\n",
      "Epoch 256 of 500 took 1.061s\n",
      "  training loss:\t\t2.301005\n",
      "Epoch 257 of 500 took 1.072s\n",
      "  training loss:\t\t2.301016\n",
      "Epoch 258 of 500 took 1.062s\n",
      "  training loss:\t\t2.301037\n",
      "Epoch 259 of 500 took 1.050s\n",
      "  training loss:\t\t2.301025\n",
      "Epoch 260 of 500 took 1.058s\n",
      "  training loss:\t\t2.301002\n",
      "Epoch 261 of 500 took 1.078s\n",
      "  training loss:\t\t2.301016\n",
      "Epoch 262 of 500 took 1.120s\n",
      "  training loss:\t\t2.301019\n",
      "Epoch 263 of 500 took 1.074s\n",
      "  training loss:\t\t2.301029\n",
      "Epoch 264 of 500 took 1.056s\n",
      "  training loss:\t\t2.301031\n",
      "Epoch 265 of 500 took 1.056s\n",
      "  training loss:\t\t2.301024\n",
      "Epoch 266 of 500 took 1.053s\n",
      "  training loss:\t\t2.301044\n",
      "Epoch 267 of 500 took 1.051s\n",
      "  training loss:\t\t2.301037\n",
      "Epoch 268 of 500 took 1.054s\n",
      "  training loss:\t\t2.301039\n",
      "Epoch 269 of 500 took 1.116s\n",
      "  training loss:\t\t2.301012\n",
      "Epoch 270 of 500 took 1.123s\n",
      "  training loss:\t\t2.301044\n",
      "Epoch 271 of 500 took 1.125s\n",
      "  training loss:\t\t2.301011\n",
      "Epoch 272 of 500 took 1.089s\n",
      "  training loss:\t\t2.301048\n",
      "Epoch 273 of 500 took 1.135s\n",
      "  training loss:\t\t2.301063\n",
      "Epoch 274 of 500 took 1.134s\n",
      "  training loss:\t\t2.301086\n",
      "Epoch 275 of 500 took 1.098s\n",
      "  training loss:\t\t2.301016\n",
      "Epoch 276 of 500 took 1.060s\n",
      "  training loss:\t\t2.301050\n",
      "Epoch 277 of 500 took 1.058s\n",
      "  training loss:\t\t2.301000\n",
      "Epoch 278 of 500 took 1.057s\n",
      "  training loss:\t\t2.301059\n",
      "Epoch 279 of 500 took 1.054s\n",
      "  training loss:\t\t2.301039\n",
      "Epoch 280 of 500 took 1.053s\n",
      "  training loss:\t\t2.301019\n",
      "Epoch 281 of 500 took 1.053s\n",
      "  training loss:\t\t2.301060\n",
      "Epoch 282 of 500 took 1.053s\n",
      "  training loss:\t\t2.301063\n",
      "Epoch 283 of 500 took 1.052s\n",
      "  training loss:\t\t2.301086\n",
      "Epoch 284 of 500 took 1.051s\n",
      "  training loss:\t\t2.301032\n",
      "Epoch 285 of 500 took 1.055s\n",
      "  training loss:\t\t2.301040\n",
      "Epoch 286 of 500 took 1.051s\n",
      "  training loss:\t\t2.301071\n",
      "Epoch 287 of 500 took 1.057s\n",
      "  training loss:\t\t2.301071\n",
      "Epoch 288 of 500 took 1.054s\n",
      "  training loss:\t\t2.301032\n",
      "Epoch 289 of 500 took 1.071s\n",
      "  training loss:\t\t2.300988\n",
      "Epoch 290 of 500 took 1.053s\n",
      "  training loss:\t\t2.300971\n",
      "Epoch 291 of 500 took 1.059s\n",
      "  training loss:\t\t2.301018\n",
      "Epoch 292 of 500 took 1.054s\n",
      "  training loss:\t\t2.301061\n",
      "Epoch 293 of 500 took 1.107s\n",
      "  training loss:\t\t2.301021\n",
      "Epoch 294 of 500 took 1.058s\n",
      "  training loss:\t\t2.301013\n",
      "Epoch 295 of 500 took 1.112s\n",
      "  training loss:\t\t2.301036\n",
      "Epoch 296 of 500 took 1.120s\n",
      "  training loss:\t\t2.301028\n",
      "Epoch 297 of 500 took 1.115s\n",
      "  training loss:\t\t2.300976\n",
      "Epoch 298 of 500 took 1.096s\n",
      "  training loss:\t\t2.301046\n",
      "Epoch 299 of 500 took 1.075s\n",
      "  training loss:\t\t2.300994\n",
      "Epoch 300 of 500 took 1.079s\n",
      "  training loss:\t\t2.301008\n",
      "Epoch 301 of 500 took 1.069s\n",
      "  training loss:\t\t2.300959\n",
      "Epoch 302 of 500 took 1.057s\n",
      "  training loss:\t\t2.301010\n",
      "Epoch 303 of 500 took 1.055s\n",
      "  training loss:\t\t2.301053\n",
      "Epoch 304 of 500 took 1.058s\n",
      "  training loss:\t\t2.301010\n",
      "Epoch 305 of 500 took 1.054s\n",
      "  training loss:\t\t2.301048\n",
      "Epoch 306 of 500 took 1.056s\n",
      "  training loss:\t\t2.301054\n",
      "Epoch 307 of 500 took 1.056s\n",
      "  training loss:\t\t2.301027\n",
      "Epoch 308 of 500 took 1.084s\n",
      "  training loss:\t\t2.301060\n",
      "Epoch 309 of 500 took 1.075s\n",
      "  training loss:\t\t2.301009\n",
      "Epoch 310 of 500 took 1.053s\n",
      "  training loss:\t\t2.301032\n",
      "Epoch 311 of 500 took 1.058s\n",
      "  training loss:\t\t2.301025\n",
      "Epoch 312 of 500 took 1.051s\n",
      "  training loss:\t\t2.301044\n",
      "Epoch 313 of 500 took 1.064s\n",
      "  training loss:\t\t2.301060\n",
      "Epoch 314 of 500 took 1.087s\n",
      "  training loss:\t\t2.301035\n",
      "Epoch 315 of 500 took 1.076s\n",
      "  training loss:\t\t2.301033\n",
      "Epoch 316 of 500 took 1.053s\n",
      "  training loss:\t\t2.301033\n",
      "Epoch 317 of 500 took 1.059s\n",
      "  training loss:\t\t2.301022\n",
      "Epoch 318 of 500 took 1.054s\n",
      "  training loss:\t\t2.301055\n",
      "Epoch 319 of 500 took 1.065s\n",
      "  training loss:\t\t2.301033\n",
      "Epoch 320 of 500 took 1.052s\n",
      "  training loss:\t\t2.301028\n",
      "Epoch 321 of 500 took 1.074s\n",
      "  training loss:\t\t2.301029\n",
      "Epoch 322 of 500 took 1.059s\n",
      "  training loss:\t\t2.301030\n",
      "Epoch 323 of 500 took 1.072s\n",
      "  training loss:\t\t2.301033\n",
      "Epoch 324 of 500 took 1.056s\n",
      "  training loss:\t\t2.301015\n",
      "Epoch 325 of 500 took 1.054s\n",
      "  training loss:\t\t2.301035\n",
      "Epoch 326 of 500 took 1.078s\n",
      "  training loss:\t\t2.301019\n",
      "Epoch 327 of 500 took 1.065s\n",
      "  training loss:\t\t2.301038\n",
      "Epoch 328 of 500 took 1.053s\n",
      "  training loss:\t\t2.301016\n",
      "Epoch 329 of 500 took 1.066s\n",
      "  training loss:\t\t2.301059\n",
      "Epoch 330 of 500 took 1.066s\n",
      "  training loss:\t\t2.301030\n",
      "Epoch 331 of 500 took 1.058s\n",
      "  training loss:\t\t2.301029\n",
      "Epoch 332 of 500 took 1.062s\n",
      "  training loss:\t\t2.301012\n",
      "Epoch 333 of 500 took 1.053s\n",
      "  training loss:\t\t2.301081\n",
      "Epoch 334 of 500 took 1.053s\n",
      "  training loss:\t\t2.301048\n",
      "Epoch 335 of 500 took 1.058s\n",
      "  training loss:\t\t2.301022\n",
      "Epoch 336 of 500 took 1.062s\n",
      "  training loss:\t\t2.301058\n",
      "Epoch 337 of 500 took 1.075s\n",
      "  training loss:\t\t2.301034\n",
      "Epoch 338 of 500 took 1.050s\n",
      "  training loss:\t\t2.301007\n",
      "Epoch 339 of 500 took 1.059s\n",
      "  training loss:\t\t2.301038\n",
      "Epoch 340 of 500 took 1.063s\n",
      "  training loss:\t\t2.301059\n",
      "Epoch 341 of 500 took 1.048s\n",
      "  training loss:\t\t2.301025\n",
      "Epoch 342 of 500 took 1.079s\n",
      "  training loss:\t\t2.301049\n",
      "Epoch 343 of 500 took 1.064s\n",
      "  training loss:\t\t2.301031\n",
      "Epoch 344 of 500 took 1.058s\n",
      "  training loss:\t\t2.301038\n",
      "Epoch 345 of 500 took 1.061s\n",
      "  training loss:\t\t2.301022\n",
      "Epoch 346 of 500 took 1.064s\n",
      "  training loss:\t\t2.300990\n",
      "Epoch 347 of 500 took 1.055s\n",
      "  training loss:\t\t2.301019\n",
      "Epoch 348 of 500 took 1.056s\n",
      "  training loss:\t\t2.301061\n",
      "Epoch 349 of 500 took 1.055s\n",
      "  training loss:\t\t2.301031\n",
      "Epoch 350 of 500 took 1.053s\n",
      "  training loss:\t\t2.301040\n",
      "Epoch 351 of 500 took 1.054s\n",
      "  training loss:\t\t2.301081\n",
      "Epoch 352 of 500 took 1.101s\n",
      "  training loss:\t\t2.301054\n",
      "Epoch 353 of 500 took 1.128s\n",
      "  training loss:\t\t2.301033\n",
      "Epoch 354 of 500 took 1.085s\n",
      "  training loss:\t\t2.301027\n",
      "Epoch 355 of 500 took 1.116s\n",
      "  training loss:\t\t2.301065\n",
      "Epoch 356 of 500 took 1.104s\n",
      "  training loss:\t\t2.301056\n",
      "Epoch 357 of 500 took 1.065s\n",
      "  training loss:\t\t2.300999\n",
      "Epoch 358 of 500 took 1.060s\n",
      "  training loss:\t\t2.301047\n",
      "Epoch 359 of 500 took 1.121s\n",
      "  training loss:\t\t2.301054\n",
      "Epoch 360 of 500 took 1.095s\n",
      "  training loss:\t\t2.301037\n",
      "Epoch 361 of 500 took 1.062s\n",
      "  training loss:\t\t2.301050\n",
      "Epoch 362 of 500 took 1.054s\n",
      "  training loss:\t\t2.301020\n",
      "Epoch 363 of 500 took 1.050s\n",
      "  training loss:\t\t2.301023\n",
      "Epoch 364 of 500 took 1.061s\n",
      "  training loss:\t\t2.301015\n",
      "Epoch 365 of 500 took 1.052s\n",
      "  training loss:\t\t2.301019\n",
      "Epoch 366 of 500 took 1.056s\n",
      "  training loss:\t\t2.301042\n",
      "Epoch 367 of 500 took 1.053s\n",
      "  training loss:\t\t2.301043\n",
      "Epoch 368 of 500 took 1.054s\n",
      "  training loss:\t\t2.301032\n",
      "Epoch 369 of 500 took 1.058s\n",
      "  training loss:\t\t2.301061\n",
      "Epoch 370 of 500 took 1.050s\n",
      "  training loss:\t\t2.301031\n",
      "Epoch 371 of 500 took 1.214s\n",
      "  training loss:\t\t2.301039\n",
      "Epoch 372 of 500 took 1.060s\n",
      "  training loss:\t\t2.301023\n",
      "Epoch 373 of 500 took 1.051s\n",
      "  training loss:\t\t2.301047\n",
      "Epoch 374 of 500 took 1.049s\n",
      "  training loss:\t\t2.301029\n",
      "Epoch 375 of 500 took 1.051s\n",
      "  training loss:\t\t2.301054\n",
      "Epoch 376 of 500 took 1.048s\n",
      "  training loss:\t\t2.301031\n",
      "Epoch 377 of 500 took 1.048s\n",
      "  training loss:\t\t2.301048\n",
      "Epoch 378 of 500 took 1.047s\n",
      "  training loss:\t\t2.301015\n",
      "Epoch 379 of 500 took 1.050s\n",
      "  training loss:\t\t2.301029\n",
      "Epoch 380 of 500 took 1.059s\n",
      "  training loss:\t\t2.301044\n",
      "Epoch 381 of 500 took 1.064s\n",
      "  training loss:\t\t2.301017\n",
      "Epoch 382 of 500 took 1.125s\n",
      "  training loss:\t\t2.301059\n",
      "Epoch 383 of 500 took 1.089s\n",
      "  training loss:\t\t2.301009\n",
      "Epoch 384 of 500 took 1.056s\n",
      "  training loss:\t\t2.301038\n",
      "Epoch 385 of 500 took 1.056s\n",
      "  training loss:\t\t2.301067\n",
      "Epoch 386 of 500 took 1.059s\n",
      "  training loss:\t\t2.301024\n",
      "Epoch 387 of 500 took 1.067s\n",
      "  training loss:\t\t2.301024\n",
      "Epoch 388 of 500 took 1.092s\n",
      "  training loss:\t\t2.301013\n",
      "Epoch 389 of 500 took 1.163s\n",
      "  training loss:\t\t2.301031\n",
      "Epoch 390 of 500 took 1.124s\n",
      "  training loss:\t\t2.301038\n",
      "Epoch 391 of 500 took 1.109s\n",
      "  training loss:\t\t2.301056\n",
      "Epoch 392 of 500 took 1.133s\n",
      "  training loss:\t\t2.301037\n",
      "Epoch 393 of 500 took 1.149s\n",
      "  training loss:\t\t2.301035\n",
      "Epoch 394 of 500 took 1.102s\n",
      "  training loss:\t\t2.301041\n",
      "Epoch 395 of 500 took 1.196s\n",
      "  training loss:\t\t2.301041\n",
      "Epoch 396 of 500 took 1.200s\n",
      "  training loss:\t\t2.301029\n",
      "Epoch 397 of 500 took 1.130s\n",
      "  training loss:\t\t2.301043\n",
      "Epoch 398 of 500 took 1.057s\n",
      "  training loss:\t\t2.301024\n",
      "Epoch 399 of 500 took 1.219s\n",
      "  training loss:\t\t2.301021\n",
      "Epoch 400 of 500 took 1.065s\n",
      "  training loss:\t\t2.301029\n",
      "Epoch 401 of 500 took 1.055s\n",
      "  training loss:\t\t2.301037\n",
      "Epoch 402 of 500 took 1.057s\n",
      "  training loss:\t\t2.301032\n",
      "Epoch 403 of 500 took 1.053s\n",
      "  training loss:\t\t2.301041\n",
      "Epoch 404 of 500 took 1.213s\n",
      "  training loss:\t\t2.301034\n",
      "Epoch 405 of 500 took 1.205s\n",
      "  training loss:\t\t2.301039\n",
      "Epoch 406 of 500 took 1.175s\n",
      "  training loss:\t\t2.301018\n",
      "Epoch 407 of 500 took 1.153s\n",
      "  training loss:\t\t2.301012\n",
      "Epoch 408 of 500 took 1.206s\n",
      "  training loss:\t\t2.301035\n",
      "Epoch 409 of 500 took 1.125s\n",
      "  training loss:\t\t2.301006\n",
      "Epoch 410 of 500 took 1.146s\n",
      "  training loss:\t\t2.301037\n",
      "Epoch 411 of 500 took 1.183s\n",
      "  training loss:\t\t2.301086\n",
      "Epoch 412 of 500 took 1.072s\n",
      "  training loss:\t\t2.301046\n",
      "Epoch 413 of 500 took 1.173s\n",
      "  training loss:\t\t2.300995\n",
      "Epoch 414 of 500 took 1.101s\n",
      "  training loss:\t\t2.301045\n",
      "Epoch 415 of 500 took 1.211s\n",
      "  training loss:\t\t2.301043\n",
      "Epoch 416 of 500 took 1.234s\n",
      "  training loss:\t\t2.301029\n",
      "Epoch 417 of 500 took 1.226s\n",
      "  training loss:\t\t2.301068\n",
      "Epoch 418 of 500 took 1.193s\n",
      "  training loss:\t\t2.301018\n",
      "Epoch 419 of 500 took 1.060s\n",
      "  training loss:\t\t2.301029\n",
      "Epoch 420 of 500 took 1.054s\n",
      "  training loss:\t\t2.301020\n",
      "Epoch 421 of 500 took 1.176s\n",
      "  training loss:\t\t2.301013\n",
      "Epoch 422 of 500 took 1.101s\n",
      "  training loss:\t\t2.301021\n",
      "Epoch 423 of 500 took 1.110s\n",
      "  training loss:\t\t2.301011\n",
      "Epoch 424 of 500 took 1.247s\n",
      "  training loss:\t\t2.301034\n",
      "Epoch 425 of 500 took 1.202s\n",
      "  training loss:\t\t2.300989\n",
      "Epoch 426 of 500 took 1.124s\n",
      "  training loss:\t\t2.301030\n",
      "Epoch 427 of 500 took 1.155s\n",
      "  training loss:\t\t2.301051\n",
      "Epoch 428 of 500 took 1.189s\n",
      "  training loss:\t\t2.301048\n",
      "Epoch 429 of 500 took 1.229s\n",
      "  training loss:\t\t2.301040\n",
      "Epoch 430 of 500 took 1.172s\n",
      "  training loss:\t\t2.301022\n",
      "Epoch 431 of 500 took 1.107s\n",
      "  training loss:\t\t2.301001\n",
      "Epoch 432 of 500 took 1.229s\n",
      "  training loss:\t\t2.301013\n",
      "Epoch 433 of 500 took 1.117s\n",
      "  training loss:\t\t2.301051\n",
      "Epoch 434 of 500 took 1.161s\n",
      "  training loss:\t\t2.301070\n",
      "Epoch 435 of 500 took 1.145s\n",
      "  training loss:\t\t2.301018\n",
      "Epoch 436 of 500 took 1.121s\n",
      "  training loss:\t\t2.301058\n",
      "Epoch 437 of 500 took 1.197s\n",
      "  training loss:\t\t2.300980\n",
      "Epoch 438 of 500 took 1.099s\n",
      "  training loss:\t\t2.300999\n",
      "Epoch 439 of 500 took 1.166s\n",
      "  training loss:\t\t2.301002\n",
      "Epoch 440 of 500 took 1.064s\n",
      "  training loss:\t\t2.300994\n",
      "Epoch 441 of 500 took 1.055s\n",
      "  training loss:\t\t2.301026\n",
      "Epoch 442 of 500 took 1.057s\n",
      "  training loss:\t\t2.301026\n",
      "Epoch 443 of 500 took 1.062s\n",
      "  training loss:\t\t2.301032\n",
      "Epoch 444 of 500 took 1.071s\n",
      "  training loss:\t\t2.301024\n",
      "Epoch 445 of 500 took 1.058s\n",
      "  training loss:\t\t2.300990\n",
      "Epoch 446 of 500 took 1.110s\n",
      "  training loss:\t\t2.301030\n",
      "Epoch 447 of 500 took 1.210s\n",
      "  training loss:\t\t2.301018\n",
      "Epoch 448 of 500 took 1.173s\n",
      "  training loss:\t\t2.301030\n",
      "Epoch 449 of 500 took 1.228s\n",
      "  training loss:\t\t2.301025\n",
      "Epoch 450 of 500 took 1.131s\n",
      "  training loss:\t\t2.301024\n",
      "Epoch 451 of 500 took 1.089s\n",
      "  training loss:\t\t2.301042\n",
      "Epoch 452 of 500 took 1.173s\n",
      "  training loss:\t\t2.301017\n",
      "Epoch 453 of 500 took 1.140s\n",
      "  training loss:\t\t2.300995\n",
      "Epoch 454 of 500 took 1.194s\n",
      "  training loss:\t\t2.301036\n",
      "Epoch 455 of 500 took 1.060s\n",
      "  training loss:\t\t2.301035\n",
      "Epoch 456 of 500 took 1.054s\n",
      "  training loss:\t\t2.301067\n",
      "Epoch 457 of 500 took 1.091s\n",
      "  training loss:\t\t2.301050\n",
      "Epoch 458 of 500 took 1.069s\n",
      "  training loss:\t\t2.301029\n",
      "Epoch 459 of 500 took 1.070s\n",
      "  training loss:\t\t2.301047\n",
      "Epoch 460 of 500 took 1.115s\n",
      "  training loss:\t\t2.301037\n",
      "Epoch 461 of 500 took 1.147s\n",
      "  training loss:\t\t2.301001\n",
      "Epoch 462 of 500 took 1.213s\n",
      "  training loss:\t\t2.301040\n",
      "Epoch 463 of 500 took 1.067s\n",
      "  training loss:\t\t2.301033\n",
      "Epoch 464 of 500 took 1.063s\n",
      "  training loss:\t\t2.301047\n",
      "Epoch 465 of 500 took 1.053s\n",
      "  training loss:\t\t2.301011\n",
      "Epoch 466 of 500 took 1.058s\n",
      "  training loss:\t\t2.301031\n",
      "Epoch 467 of 500 took 1.056s\n",
      "  training loss:\t\t2.301010\n",
      "Epoch 468 of 500 took 1.064s\n",
      "  training loss:\t\t2.301016\n",
      "Epoch 469 of 500 took 1.058s\n",
      "  training loss:\t\t2.301031\n",
      "Epoch 470 of 500 took 1.070s\n",
      "  training loss:\t\t2.301025\n",
      "Epoch 471 of 500 took 1.210s\n",
      "  training loss:\t\t2.301032\n",
      "Epoch 472 of 500 took 1.137s\n",
      "  training loss:\t\t2.300998\n",
      "Epoch 473 of 500 took 1.189s\n",
      "  training loss:\t\t2.301036\n",
      "Epoch 474 of 500 took 1.218s\n",
      "  training loss:\t\t2.301009\n",
      "Epoch 475 of 500 took 1.079s\n",
      "  training loss:\t\t2.301057\n",
      "Epoch 476 of 500 took 1.066s\n",
      "  training loss:\t\t2.301033\n",
      "Epoch 477 of 500 took 1.072s\n",
      "  training loss:\t\t2.301018\n",
      "Epoch 478 of 500 took 1.264s\n",
      "  training loss:\t\t2.301034\n",
      "Epoch 479 of 500 took 1.273s\n",
      "  training loss:\t\t2.301027\n",
      "Epoch 480 of 500 took 1.072s\n",
      "  training loss:\t\t2.301027\n",
      "Epoch 481 of 500 took 1.051s\n",
      "  training loss:\t\t2.301034\n",
      "Epoch 482 of 500 took 1.053s\n",
      "  training loss:\t\t2.301052\n",
      "Epoch 483 of 500 took 1.061s\n",
      "  training loss:\t\t2.301020\n",
      "Epoch 484 of 500 took 1.051s\n",
      "  training loss:\t\t2.301009\n",
      "Epoch 485 of 500 took 1.067s\n",
      "  training loss:\t\t2.301015\n",
      "Epoch 486 of 500 took 1.229s\n",
      "  training loss:\t\t2.300994\n",
      "Epoch 487 of 500 took 1.215s\n",
      "  training loss:\t\t2.301025\n",
      "Epoch 488 of 500 took 1.058s\n",
      "  training loss:\t\t2.301021\n",
      "Epoch 489 of 500 took 1.071s\n",
      "  training loss:\t\t2.301014\n",
      "Epoch 490 of 500 took 1.060s\n",
      "  training loss:\t\t2.301006\n",
      "Epoch 491 of 500 took 1.057s\n",
      "  training loss:\t\t2.301033\n",
      "Epoch 492 of 500 took 1.057s\n",
      "  training loss:\t\t2.301063\n",
      "Epoch 493 of 500 took 1.054s\n",
      "  training loss:\t\t2.301002\n",
      "Epoch 494 of 500 took 1.055s\n",
      "  training loss:\t\t2.301059\n",
      "Epoch 495 of 500 took 1.051s\n",
      "  training loss:\t\t2.301007\n",
      "Epoch 496 of 500 took 1.055s\n",
      "  training loss:\t\t2.301017\n",
      "Epoch 497 of 500 took 1.051s\n",
      "  training loss:\t\t2.301010\n",
      "Epoch 498 of 500 took 1.047s\n",
      "  training loss:\t\t2.300989\n",
      "Epoch 499 of 500 took 1.049s\n",
      "  training loss:\t\t2.301021\n",
      "Epoch 500 of 500 took 1.049s\n",
      "  training loss:\t\t2.301029\n",
      "Final results:\n",
      "  test loss:\t\t\t2.301004\n",
      "  test accuracy:\t\t11.35 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "input_var = T.matrix('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "network = build_NIN_model(input_var, 1024, 64, 8)\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "#l2_penalty = regularize_network_params(network, l2)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.adagrad(loss, params, learning_rate=0.01)\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                    dtype=theano.config.floatX)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "        # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, batch_size, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "final_acc = test_acc / test_batches\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    final_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
