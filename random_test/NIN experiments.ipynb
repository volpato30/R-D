{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: TITAN X (Pascal) (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "from logistic_sgd import load_data\n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
    "from lasagne.regularization import regularize_network_params, l2, l1\n",
    "import gzip\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "f = gzip.open('/home/rui/Downloads/mnist.pkl.gz', 'rb')\n",
    "try:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "except:\n",
    "    train_set, valid_set, test_set = pickle.load(f)\n",
    "f.close()\n",
    "X_train, y_train = train_set\n",
    "y_train = np.asarray(y_train, dtype = np.int32)\n",
    "X_test, y_test = test_set\n",
    "y_test = np.asarray(y_test, dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with 2 fully connected hidden layers, each layer has 1024 hidden units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(input_var, hidden_neurons=1024):\n",
    "    l_in = InputLayer(shape=(None, 784), input_var=input_var)\n",
    "    l_hidden1 = DenseLayer(l_in, num_units=hidden_neurons, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    l_hidden2 = DenseLayer(l_hidden1, num_units=hidden_neurons, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    l_out = DenseLayer(lasagne.layers.DropoutLayer(l_hidden2), num_units=10, nonlinearity=lasagne.nonlinearities.softmax, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with adagrad algorithm, weight decay with parameter 1e-4\n",
    "\n",
    "## final accuracy: 97.77%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 200 took 0.876s\n",
      "  training loss:\t\t20.089788\n",
      "Epoch 2 of 200 took 0.818s\n",
      "  training loss:\t\t1.018199\n",
      "Epoch 3 of 200 took 0.815s\n",
      "  training loss:\t\t0.859098\n",
      "Epoch 4 of 200 took 0.816s\n",
      "  training loss:\t\t0.774754\n",
      "Epoch 5 of 200 took 0.816s\n",
      "  training loss:\t\t0.715267\n",
      "Epoch 6 of 200 took 0.814s\n",
      "  training loss:\t\t0.671893\n",
      "Epoch 7 of 200 took 0.821s\n",
      "  training loss:\t\t0.635314\n",
      "Epoch 8 of 200 took 0.819s\n",
      "  training loss:\t\t0.604176\n",
      "Epoch 9 of 200 took 0.817s\n",
      "  training loss:\t\t0.575619\n",
      "Epoch 10 of 200 took 0.815s\n",
      "  training loss:\t\t0.551321\n",
      "Epoch 11 of 200 took 0.816s\n",
      "  training loss:\t\t0.529764\n",
      "Epoch 12 of 200 took 0.815s\n",
      "  training loss:\t\t0.510364\n",
      "Epoch 13 of 200 took 0.816s\n",
      "  training loss:\t\t0.493556\n",
      "Epoch 14 of 200 took 0.814s\n",
      "  training loss:\t\t0.479244\n",
      "Epoch 15 of 200 took 0.814s\n",
      "  training loss:\t\t0.462639\n",
      "Epoch 16 of 200 took 0.820s\n",
      "  training loss:\t\t0.450371\n",
      "Epoch 17 of 200 took 0.832s\n",
      "  training loss:\t\t0.436246\n",
      "Epoch 18 of 200 took 0.814s\n",
      "  training loss:\t\t0.425166\n",
      "Epoch 19 of 200 took 0.845s\n",
      "  training loss:\t\t0.413011\n",
      "Epoch 20 of 200 took 0.838s\n",
      "  training loss:\t\t0.403946\n",
      "Epoch 21 of 200 took 0.822s\n",
      "  training loss:\t\t0.393429\n",
      "Epoch 22 of 200 took 0.849s\n",
      "  training loss:\t\t0.383972\n",
      "Epoch 23 of 200 took 0.823s\n",
      "  training loss:\t\t0.375789\n",
      "Epoch 24 of 200 took 0.827s\n",
      "  training loss:\t\t0.367740\n",
      "Epoch 25 of 200 took 0.825s\n",
      "  training loss:\t\t0.359904\n",
      "Epoch 26 of 200 took 0.823s\n",
      "  training loss:\t\t0.351064\n",
      "Epoch 27 of 200 took 0.826s\n",
      "  training loss:\t\t0.344229\n",
      "Epoch 28 of 200 took 0.827s\n",
      "  training loss:\t\t0.338223\n",
      "Epoch 29 of 200 took 0.822s\n",
      "  training loss:\t\t0.329315\n",
      "Epoch 30 of 200 took 0.825s\n",
      "  training loss:\t\t0.325491\n",
      "Epoch 31 of 200 took 0.828s\n",
      "  training loss:\t\t0.317731\n",
      "Epoch 32 of 200 took 0.833s\n",
      "  training loss:\t\t0.311832\n",
      "Epoch 33 of 200 took 0.832s\n",
      "  training loss:\t\t0.306838\n",
      "Epoch 34 of 200 took 0.826s\n",
      "  training loss:\t\t0.303123\n",
      "Epoch 35 of 200 took 0.825s\n",
      "  training loss:\t\t0.298446\n",
      "Epoch 36 of 200 took 0.822s\n",
      "  training loss:\t\t0.292964\n",
      "Epoch 37 of 200 took 0.834s\n",
      "  training loss:\t\t0.286674\n",
      "Epoch 38 of 200 took 0.837s\n",
      "  training loss:\t\t0.283287\n",
      "Epoch 39 of 200 took 0.834s\n",
      "  training loss:\t\t0.278126\n",
      "Epoch 40 of 200 took 0.836s\n",
      "  training loss:\t\t0.273417\n",
      "Epoch 41 of 200 took 0.832s\n",
      "  training loss:\t\t0.270055\n",
      "Epoch 42 of 200 took 0.836s\n",
      "  training loss:\t\t0.266245\n",
      "Epoch 43 of 200 took 0.838s\n",
      "  training loss:\t\t0.262515\n",
      "Epoch 44 of 200 took 0.831s\n",
      "  training loss:\t\t0.258494\n",
      "Epoch 45 of 200 took 0.842s\n",
      "  training loss:\t\t0.254498\n",
      "Epoch 46 of 200 took 0.838s\n",
      "  training loss:\t\t0.250518\n",
      "Epoch 47 of 200 took 0.831s\n",
      "  training loss:\t\t0.247207\n",
      "Epoch 48 of 200 took 0.837s\n",
      "  training loss:\t\t0.243349\n",
      "Epoch 49 of 200 took 0.836s\n",
      "  training loss:\t\t0.241827\n",
      "Epoch 50 of 200 took 0.834s\n",
      "  training loss:\t\t0.237300\n",
      "Epoch 51 of 200 took 0.827s\n",
      "  training loss:\t\t0.235529\n",
      "Epoch 52 of 200 took 0.833s\n",
      "  training loss:\t\t0.231344\n",
      "Epoch 53 of 200 took 0.836s\n",
      "  training loss:\t\t0.229705\n",
      "Epoch 54 of 200 took 0.835s\n",
      "  training loss:\t\t0.225639\n",
      "Epoch 55 of 200 took 0.833s\n",
      "  training loss:\t\t0.223516\n",
      "Epoch 56 of 200 took 0.840s\n",
      "  training loss:\t\t0.220469\n",
      "Epoch 57 of 200 took 0.839s\n",
      "  training loss:\t\t0.217944\n",
      "Epoch 58 of 200 took 0.832s\n",
      "  training loss:\t\t0.215198\n",
      "Epoch 59 of 200 took 0.827s\n",
      "  training loss:\t\t0.212694\n",
      "Epoch 60 of 200 took 0.827s\n",
      "  training loss:\t\t0.211094\n",
      "Epoch 61 of 200 took 0.833s\n",
      "  training loss:\t\t0.209190\n",
      "Epoch 62 of 200 took 0.827s\n",
      "  training loss:\t\t0.206148\n",
      "Epoch 63 of 200 took 0.833s\n",
      "  training loss:\t\t0.204020\n",
      "Epoch 64 of 200 took 0.835s\n",
      "  training loss:\t\t0.201000\n",
      "Epoch 65 of 200 took 0.839s\n",
      "  training loss:\t\t0.200182\n",
      "Epoch 66 of 200 took 0.831s\n",
      "  training loss:\t\t0.197304\n",
      "Epoch 67 of 200 took 0.829s\n",
      "  training loss:\t\t0.195070\n",
      "Epoch 68 of 200 took 0.839s\n",
      "  training loss:\t\t0.193025\n",
      "Epoch 69 of 200 took 0.829s\n",
      "  training loss:\t\t0.191030\n",
      "Epoch 70 of 200 took 0.840s\n",
      "  training loss:\t\t0.189611\n",
      "Epoch 71 of 200 took 0.822s\n",
      "  training loss:\t\t0.187811\n",
      "Epoch 72 of 200 took 0.832s\n",
      "  training loss:\t\t0.185409\n",
      "Epoch 73 of 200 took 0.827s\n",
      "  training loss:\t\t0.183708\n",
      "Epoch 74 of 200 took 0.829s\n",
      "  training loss:\t\t0.181194\n",
      "Epoch 75 of 200 took 0.826s\n",
      "  training loss:\t\t0.180305\n",
      "Epoch 76 of 200 took 0.822s\n",
      "  training loss:\t\t0.178913\n",
      "Epoch 77 of 200 took 0.821s\n",
      "  training loss:\t\t0.176291\n",
      "Epoch 78 of 200 took 0.828s\n",
      "  training loss:\t\t0.175507\n",
      "Epoch 79 of 200 took 0.823s\n",
      "  training loss:\t\t0.173665\n",
      "Epoch 80 of 200 took 0.821s\n",
      "  training loss:\t\t0.171384\n",
      "Epoch 81 of 200 took 0.828s\n",
      "  training loss:\t\t0.169890\n",
      "Epoch 82 of 200 took 0.826s\n",
      "  training loss:\t\t0.169023\n",
      "Epoch 83 of 200 took 0.819s\n",
      "  training loss:\t\t0.167099\n",
      "Epoch 84 of 200 took 0.823s\n",
      "  training loss:\t\t0.165237\n",
      "Epoch 85 of 200 took 0.831s\n",
      "  training loss:\t\t0.163551\n",
      "Epoch 86 of 200 took 0.829s\n",
      "  training loss:\t\t0.162825\n",
      "Epoch 87 of 200 took 0.825s\n",
      "  training loss:\t\t0.161245\n",
      "Epoch 88 of 200 took 0.828s\n",
      "  training loss:\t\t0.160380\n",
      "Epoch 89 of 200 took 0.827s\n",
      "  training loss:\t\t0.157874\n",
      "Epoch 90 of 200 took 0.835s\n",
      "  training loss:\t\t0.156890\n",
      "Epoch 91 of 200 took 0.825s\n",
      "  training loss:\t\t0.154935\n",
      "Epoch 92 of 200 took 0.825s\n",
      "  training loss:\t\t0.153963\n",
      "Epoch 93 of 200 took 0.828s\n",
      "  training loss:\t\t0.153542\n",
      "Epoch 94 of 200 took 0.833s\n",
      "  training loss:\t\t0.151962\n",
      "Epoch 95 of 200 took 0.832s\n",
      "  training loss:\t\t0.150676\n",
      "Epoch 96 of 200 took 0.828s\n",
      "  training loss:\t\t0.148688\n",
      "Epoch 97 of 200 took 0.832s\n",
      "  training loss:\t\t0.148911\n",
      "Epoch 98 of 200 took 0.834s\n",
      "  training loss:\t\t0.147233\n",
      "Epoch 99 of 200 took 0.824s\n",
      "  training loss:\t\t0.145915\n",
      "Epoch 100 of 200 took 0.829s\n",
      "  training loss:\t\t0.144680\n",
      "Epoch 101 of 200 took 0.835s\n",
      "  training loss:\t\t0.143793\n",
      "Epoch 102 of 200 took 0.869s\n",
      "  training loss:\t\t0.142254\n",
      "Epoch 103 of 200 took 0.869s\n",
      "  training loss:\t\t0.141335\n",
      "Epoch 104 of 200 took 0.907s\n",
      "  training loss:\t\t0.140552\n",
      "Epoch 105 of 200 took 0.837s\n",
      "  training loss:\t\t0.139531\n",
      "Epoch 106 of 200 took 0.836s\n",
      "  training loss:\t\t0.138298\n",
      "Epoch 107 of 200 took 0.829s\n",
      "  training loss:\t\t0.137095\n",
      "Epoch 108 of 200 took 0.825s\n",
      "  training loss:\t\t0.136195\n",
      "Epoch 109 of 200 took 0.829s\n",
      "  training loss:\t\t0.134737\n",
      "Epoch 110 of 200 took 0.826s\n",
      "  training loss:\t\t0.134664\n",
      "Epoch 111 of 200 took 0.826s\n",
      "  training loss:\t\t0.133620\n",
      "Epoch 112 of 200 took 0.824s\n",
      "  training loss:\t\t0.132031\n",
      "Epoch 113 of 200 took 0.825s\n",
      "  training loss:\t\t0.130888\n",
      "Epoch 114 of 200 took 0.830s\n",
      "  training loss:\t\t0.129670\n",
      "Epoch 115 of 200 took 0.834s\n",
      "  training loss:\t\t0.129858\n",
      "Epoch 116 of 200 took 0.835s\n",
      "  training loss:\t\t0.129393\n",
      "Epoch 117 of 200 took 0.826s\n",
      "  training loss:\t\t0.128073\n",
      "Epoch 118 of 200 took 0.823s\n",
      "  training loss:\t\t0.127043\n",
      "Epoch 119 of 200 took 0.823s\n",
      "  training loss:\t\t0.125804\n",
      "Epoch 120 of 200 took 0.826s\n",
      "  training loss:\t\t0.125108\n",
      "Epoch 121 of 200 took 0.823s\n",
      "  training loss:\t\t0.124483\n",
      "Epoch 122 of 200 took 0.823s\n",
      "  training loss:\t\t0.123678\n",
      "Epoch 123 of 200 took 0.825s\n",
      "  training loss:\t\t0.122706\n",
      "Epoch 124 of 200 took 0.831s\n",
      "  training loss:\t\t0.121671\n",
      "Epoch 125 of 200 took 0.830s\n",
      "  training loss:\t\t0.120883\n",
      "Epoch 126 of 200 took 0.829s\n",
      "  training loss:\t\t0.119846\n",
      "Epoch 127 of 200 took 0.828s\n",
      "  training loss:\t\t0.119430\n",
      "Epoch 128 of 200 took 0.831s\n",
      "  training loss:\t\t0.119314\n",
      "Epoch 129 of 200 took 0.822s\n",
      "  training loss:\t\t0.117955\n",
      "Epoch 130 of 200 took 0.827s\n",
      "  training loss:\t\t0.117088\n",
      "Epoch 131 of 200 took 0.830s\n",
      "  training loss:\t\t0.116299\n",
      "Epoch 132 of 200 took 0.831s\n",
      "  training loss:\t\t0.115997\n",
      "Epoch 133 of 200 took 0.837s\n",
      "  training loss:\t\t0.115637\n",
      "Epoch 134 of 200 took 0.824s\n",
      "  training loss:\t\t0.114281\n",
      "Epoch 135 of 200 took 0.832s\n",
      "  training loss:\t\t0.113813\n",
      "Epoch 136 of 200 took 0.830s\n",
      "  training loss:\t\t0.113205\n",
      "Epoch 137 of 200 took 0.830s\n",
      "  training loss:\t\t0.112290\n",
      "Epoch 138 of 200 took 0.822s\n",
      "  training loss:\t\t0.111410\n",
      "Epoch 139 of 200 took 0.827s\n",
      "  training loss:\t\t0.110545\n",
      "Epoch 140 of 200 took 0.827s\n",
      "  training loss:\t\t0.110360\n",
      "Epoch 141 of 200 took 0.835s\n",
      "  training loss:\t\t0.109858\n",
      "Epoch 142 of 200 took 0.833s\n",
      "  training loss:\t\t0.109222\n",
      "Epoch 143 of 200 took 0.835s\n",
      "  training loss:\t\t0.108643\n",
      "Epoch 144 of 200 took 0.827s\n",
      "  training loss:\t\t0.108190\n",
      "Epoch 145 of 200 took 0.832s\n",
      "  training loss:\t\t0.108030\n",
      "Epoch 146 of 200 took 0.822s\n",
      "  training loss:\t\t0.106467\n",
      "Epoch 147 of 200 took 0.832s\n",
      "  training loss:\t\t0.105890\n",
      "Epoch 148 of 200 took 0.829s\n",
      "  training loss:\t\t0.105416\n",
      "Epoch 149 of 200 took 0.824s\n",
      "  training loss:\t\t0.104759\n",
      "Epoch 150 of 200 took 0.826s\n",
      "  training loss:\t\t0.104337\n",
      "Epoch 151 of 200 took 0.826s\n",
      "  training loss:\t\t0.104282\n",
      "Epoch 152 of 200 took 0.823s\n",
      "  training loss:\t\t0.103085\n",
      "Epoch 153 of 200 took 0.832s\n",
      "  training loss:\t\t0.102722\n",
      "Epoch 154 of 200 took 0.823s\n",
      "  training loss:\t\t0.102289\n",
      "Epoch 155 of 200 took 0.826s\n",
      "  training loss:\t\t0.101737\n",
      "Epoch 156 of 200 took 0.873s\n",
      "  training loss:\t\t0.100931\n",
      "Epoch 157 of 200 took 0.915s\n",
      "  training loss:\t\t0.100832\n",
      "Epoch 158 of 200 took 0.872s\n",
      "  training loss:\t\t0.101015\n",
      "Epoch 159 of 200 took 0.875s\n",
      "  training loss:\t\t0.099759\n",
      "Epoch 160 of 200 took 0.894s\n",
      "  training loss:\t\t0.098690\n",
      "Epoch 161 of 200 took 0.859s\n",
      "  training loss:\t\t0.098742\n",
      "Epoch 162 of 200 took 0.860s\n",
      "  training loss:\t\t0.098308\n",
      "Epoch 163 of 200 took 0.861s\n",
      "  training loss:\t\t0.097706\n",
      "Epoch 164 of 200 took 0.870s\n",
      "  training loss:\t\t0.097016\n",
      "Epoch 165 of 200 took 0.822s\n",
      "  training loss:\t\t0.096305\n",
      "Epoch 166 of 200 took 0.823s\n",
      "  training loss:\t\t0.096089\n",
      "Epoch 167 of 200 took 0.821s\n",
      "  training loss:\t\t0.095686\n",
      "Epoch 168 of 200 took 0.846s\n",
      "  training loss:\t\t0.094983\n",
      "Epoch 169 of 200 took 0.872s\n",
      "  training loss:\t\t0.094843\n",
      "Epoch 170 of 200 took 0.820s\n",
      "  training loss:\t\t0.094947\n",
      "Epoch 171 of 200 took 0.883s\n",
      "  training loss:\t\t0.093908\n",
      "Epoch 172 of 200 took 0.830s\n",
      "  training loss:\t\t0.093668\n",
      "Epoch 173 of 200 took 0.837s\n",
      "  training loss:\t\t0.093052\n",
      "Epoch 174 of 200 took 0.885s\n",
      "  training loss:\t\t0.093116\n",
      "Epoch 175 of 200 took 0.838s\n",
      "  training loss:\t\t0.092138\n",
      "Epoch 176 of 200 took 0.853s\n",
      "  training loss:\t\t0.091793\n",
      "Epoch 177 of 200 took 0.825s\n",
      "  training loss:\t\t0.090823\n",
      "Epoch 178 of 200 took 0.853s\n",
      "  training loss:\t\t0.090917\n",
      "Epoch 179 of 200 took 0.837s\n",
      "  training loss:\t\t0.090520\n",
      "Epoch 180 of 200 took 0.833s\n",
      "  training loss:\t\t0.090057\n",
      "Epoch 181 of 200 took 0.847s\n",
      "  training loss:\t\t0.089429\n",
      "Epoch 182 of 200 took 0.826s\n",
      "  training loss:\t\t0.088946\n",
      "Epoch 183 of 200 took 0.840s\n",
      "  training loss:\t\t0.088930\n",
      "Epoch 184 of 200 took 0.838s\n",
      "  training loss:\t\t0.088167\n",
      "Epoch 185 of 200 took 0.889s\n",
      "  training loss:\t\t0.088083\n",
      "Epoch 186 of 200 took 0.894s\n",
      "  training loss:\t\t0.087417\n",
      "Epoch 187 of 200 took 0.843s\n",
      "  training loss:\t\t0.087540\n",
      "Epoch 188 of 200 took 0.844s\n",
      "  training loss:\t\t0.087102\n",
      "Epoch 189 of 200 took 0.827s\n",
      "  training loss:\t\t0.086414\n",
      "Epoch 190 of 200 took 0.824s\n",
      "  training loss:\t\t0.086241\n",
      "Epoch 191 of 200 took 0.825s\n",
      "  training loss:\t\t0.085768\n",
      "Epoch 192 of 200 took 0.899s\n",
      "  training loss:\t\t0.085450\n",
      "Epoch 193 of 200 took 0.835s\n",
      "  training loss:\t\t0.084930\n",
      "Epoch 194 of 200 took 0.881s\n",
      "  training loss:\t\t0.084814\n",
      "Epoch 195 of 200 took 0.918s\n",
      "  training loss:\t\t0.083818\n",
      "Epoch 196 of 200 took 0.885s\n",
      "  training loss:\t\t0.084066\n",
      "Epoch 197 of 200 took 0.855s\n",
      "  training loss:\t\t0.083949\n",
      "Epoch 198 of 200 took 0.820s\n",
      "  training loss:\t\t0.083224\n",
      "Epoch 199 of 200 took 0.893s\n",
      "  training loss:\t\t0.082795\n",
      "Epoch 200 of 200 took 0.851s\n",
      "  training loss:\t\t0.082484\n",
      "Final results:\n",
      "  test loss:\t\t\t0.079049\n",
      "  test accuracy:\t\t97.77 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "input_var = T.matrix('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "network = build_model(input_var, 1024)\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "l2_penalty = regularize_network_params(network, l2)\n",
    "loss = loss.mean() + 1e-4*l2_penalty\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.adagrad(loss, params, learning_rate=0.1)\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                    dtype=theano.config.floatX)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "        # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "final_acc = test_acc / test_batches\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    final_acc * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add bottle neck(128 units) between input layer and hidden1, hidden1 and hidden2. \n",
    "\n",
    "## final accuracy: 96.74%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_NIN_model(input_var, hidden_neurons=1024, bottle_neck=128):\n",
    "    l_in = InputLayer(shape=(None, 784), input_var=input_var)\n",
    "    l_b1 = DenseLayer(l_in, num_units=bottle_neck, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    l_hidden1 = DenseLayer(l_b1, num_units=hidden_neurons, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    l_b2 = DenseLayer(l_hidden1, num_units=bottle_neck, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    l_hidden2 = DenseLayer(l_b2, num_units=hidden_neurons, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    l_out = DenseLayer(lasagne.layers.DropoutLayer(l_hidden2), num_units=10, nonlinearity=lasagne.nonlinearities.softmax, W=lasagne.init.HeNormal(gain='relu'))\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 200 took 0.591s\n",
      "  training loss:\t\t7.346979\n",
      "Epoch 2 of 200 took 0.586s\n",
      "  training loss:\t\t0.731895\n",
      "Epoch 3 of 200 took 0.602s\n",
      "  training loss:\t\t0.554173\n",
      "Epoch 4 of 200 took 0.575s\n",
      "  training loss:\t\t0.477607\n",
      "Epoch 5 of 200 took 0.574s\n",
      "  training loss:\t\t0.432261\n",
      "Epoch 6 of 200 took 0.577s\n",
      "  training loss:\t\t0.395385\n",
      "Epoch 7 of 200 took 0.579s\n",
      "  training loss:\t\t0.372792\n",
      "Epoch 8 of 200 took 0.574s\n",
      "  training loss:\t\t0.352455\n",
      "Epoch 9 of 200 took 0.574s\n",
      "  training loss:\t\t0.334723\n",
      "Epoch 10 of 200 took 0.574s\n",
      "  training loss:\t\t0.319969\n",
      "Epoch 11 of 200 took 0.575s\n",
      "  training loss:\t\t0.306710\n",
      "Epoch 12 of 200 took 0.643s\n",
      "  training loss:\t\t0.296258\n",
      "Epoch 13 of 200 took 0.577s\n",
      "  training loss:\t\t0.285162\n",
      "Epoch 14 of 200 took 0.576s\n",
      "  training loss:\t\t0.278656\n",
      "Epoch 15 of 200 took 0.578s\n",
      "  training loss:\t\t0.267407\n",
      "Epoch 16 of 200 took 0.577s\n",
      "  training loss:\t\t0.258304\n",
      "Epoch 17 of 200 took 0.575s\n",
      "  training loss:\t\t0.252724\n",
      "Epoch 18 of 200 took 0.576s\n",
      "  training loss:\t\t0.246525\n",
      "Epoch 19 of 200 took 0.578s\n",
      "  training loss:\t\t0.238498\n",
      "Epoch 20 of 200 took 0.581s\n",
      "  training loss:\t\t0.231311\n",
      "Epoch 21 of 200 took 0.581s\n",
      "  training loss:\t\t0.225959\n",
      "Epoch 22 of 200 took 0.582s\n",
      "  training loss:\t\t0.222319\n",
      "Epoch 23 of 200 took 0.626s\n",
      "  training loss:\t\t0.217587\n",
      "Epoch 24 of 200 took 0.573s\n",
      "  training loss:\t\t0.216428\n",
      "Epoch 25 of 200 took 0.573s\n",
      "  training loss:\t\t0.208401\n",
      "Epoch 26 of 200 took 0.575s\n",
      "  training loss:\t\t0.206634\n",
      "Epoch 27 of 200 took 0.575s\n",
      "  training loss:\t\t0.200541\n",
      "Epoch 28 of 200 took 0.582s\n",
      "  training loss:\t\t0.198456\n",
      "Epoch 29 of 200 took 0.626s\n",
      "  training loss:\t\t0.195184\n",
      "Epoch 30 of 200 took 0.606s\n",
      "  training loss:\t\t0.192655\n",
      "Epoch 31 of 200 took 0.614s\n",
      "  training loss:\t\t0.189826\n",
      "Epoch 32 of 200 took 0.647s\n",
      "  training loss:\t\t0.186154\n",
      "Epoch 33 of 200 took 0.575s\n",
      "  training loss:\t\t0.184393\n",
      "Epoch 34 of 200 took 0.663s\n",
      "  training loss:\t\t0.181401\n",
      "Epoch 35 of 200 took 0.653s\n",
      "  training loss:\t\t0.178654\n",
      "Epoch 36 of 200 took 0.659s\n",
      "  training loss:\t\t0.177740\n",
      "Epoch 37 of 200 took 0.589s\n",
      "  training loss:\t\t0.174863\n",
      "Epoch 38 of 200 took 0.575s\n",
      "  training loss:\t\t0.171445\n",
      "Epoch 39 of 200 took 0.631s\n",
      "  training loss:\t\t0.170485\n",
      "Epoch 40 of 200 took 0.574s\n",
      "  training loss:\t\t0.168056\n",
      "Epoch 41 of 200 took 0.576s\n",
      "  training loss:\t\t0.164948\n",
      "Epoch 42 of 200 took 0.579s\n",
      "  training loss:\t\t0.163412\n",
      "Epoch 43 of 200 took 0.578s\n",
      "  training loss:\t\t0.162169\n",
      "Epoch 44 of 200 took 0.577s\n",
      "  training loss:\t\t0.159475\n",
      "Epoch 45 of 200 took 0.576s\n",
      "  training loss:\t\t0.158447\n",
      "Epoch 46 of 200 took 0.578s\n",
      "  training loss:\t\t0.156645\n",
      "Epoch 47 of 200 took 0.635s\n",
      "  training loss:\t\t0.155376\n",
      "Epoch 48 of 200 took 0.576s\n",
      "  training loss:\t\t0.152307\n",
      "Epoch 49 of 200 took 0.577s\n",
      "  training loss:\t\t0.151071\n",
      "Epoch 50 of 200 took 0.575s\n",
      "  training loss:\t\t0.150720\n",
      "Epoch 51 of 200 took 0.589s\n",
      "  training loss:\t\t0.148670\n",
      "Epoch 52 of 200 took 0.655s\n",
      "  training loss:\t\t0.147626\n",
      "Epoch 53 of 200 took 0.620s\n",
      "  training loss:\t\t0.145292\n",
      "Epoch 54 of 200 took 0.594s\n",
      "  training loss:\t\t0.143687\n",
      "Epoch 55 of 200 took 0.619s\n",
      "  training loss:\t\t0.143543\n",
      "Epoch 56 of 200 took 0.631s\n",
      "  training loss:\t\t0.142743\n",
      "Epoch 57 of 200 took 0.650s\n",
      "  training loss:\t\t0.141427\n",
      "Epoch 58 of 200 took 0.624s\n",
      "  training loss:\t\t0.139384\n",
      "Epoch 59 of 200 took 0.632s\n",
      "  training loss:\t\t0.138200\n",
      "Epoch 60 of 200 took 0.586s\n",
      "  training loss:\t\t0.137377\n",
      "Epoch 61 of 200 took 0.581s\n",
      "  training loss:\t\t0.134925\n",
      "Epoch 62 of 200 took 0.586s\n",
      "  training loss:\t\t0.134371\n",
      "Epoch 63 of 200 took 0.580s\n",
      "  training loss:\t\t0.134149\n",
      "Epoch 64 of 200 took 0.576s\n",
      "  training loss:\t\t0.133120\n",
      "Epoch 65 of 200 took 0.579s\n",
      "  training loss:\t\t0.131897\n",
      "Epoch 66 of 200 took 0.583s\n",
      "  training loss:\t\t0.130485\n",
      "Epoch 67 of 200 took 0.580s\n",
      "  training loss:\t\t0.129290\n",
      "Epoch 68 of 200 took 0.625s\n",
      "  training loss:\t\t0.128717\n",
      "Epoch 69 of 200 took 0.640s\n",
      "  training loss:\t\t0.128273\n",
      "Epoch 70 of 200 took 0.592s\n",
      "  training loss:\t\t0.127135\n",
      "Epoch 71 of 200 took 0.579s\n",
      "  training loss:\t\t0.126707\n",
      "Epoch 72 of 200 took 0.575s\n",
      "  training loss:\t\t0.126197\n",
      "Epoch 73 of 200 took 0.579s\n",
      "  training loss:\t\t0.125502\n",
      "Epoch 74 of 200 took 0.594s\n",
      "  training loss:\t\t0.123855\n",
      "Epoch 75 of 200 took 0.580s\n",
      "  training loss:\t\t0.123451\n",
      "Epoch 76 of 200 took 0.575s\n",
      "  training loss:\t\t0.122943\n",
      "Epoch 77 of 200 took 0.596s\n",
      "  training loss:\t\t0.122299\n",
      "Epoch 78 of 200 took 0.642s\n",
      "  training loss:\t\t0.121469\n",
      "Epoch 79 of 200 took 0.603s\n",
      "  training loss:\t\t0.119893\n",
      "Epoch 80 of 200 took 0.623s\n",
      "  training loss:\t\t0.119641\n",
      "Epoch 81 of 200 took 0.609s\n",
      "  training loss:\t\t0.119347\n",
      "Epoch 82 of 200 took 0.621s\n",
      "  training loss:\t\t0.118927\n",
      "Epoch 83 of 200 took 0.602s\n",
      "  training loss:\t\t0.118498\n",
      "Epoch 84 of 200 took 0.601s\n",
      "  training loss:\t\t0.117264\n",
      "Epoch 85 of 200 took 0.621s\n",
      "  training loss:\t\t0.116543\n",
      "Epoch 86 of 200 took 0.621s\n",
      "  training loss:\t\t0.116896\n",
      "Epoch 87 of 200 took 0.637s\n",
      "  training loss:\t\t0.115720\n",
      "Epoch 88 of 200 took 0.620s\n",
      "  training loss:\t\t0.114989\n",
      "Epoch 89 of 200 took 0.612s\n",
      "  training loss:\t\t0.114404\n",
      "Epoch 90 of 200 took 0.618s\n",
      "  training loss:\t\t0.114395\n",
      "Epoch 91 of 200 took 0.587s\n",
      "  training loss:\t\t0.113254\n",
      "Epoch 92 of 200 took 0.594s\n",
      "  training loss:\t\t0.113667\n",
      "Epoch 93 of 200 took 0.626s\n",
      "  training loss:\t\t0.112222\n",
      "Epoch 94 of 200 took 0.593s\n",
      "  training loss:\t\t0.112322\n",
      "Epoch 95 of 200 took 0.594s\n",
      "  training loss:\t\t0.112031\n",
      "Epoch 96 of 200 took 0.619s\n",
      "  training loss:\t\t0.110666\n",
      "Epoch 97 of 200 took 0.598s\n",
      "  training loss:\t\t0.112367\n",
      "Epoch 98 of 200 took 0.585s\n",
      "  training loss:\t\t0.110320\n",
      "Epoch 99 of 200 took 0.600s\n",
      "  training loss:\t\t0.109998\n",
      "Epoch 100 of 200 took 0.596s\n",
      "  training loss:\t\t0.109622\n",
      "Epoch 101 of 200 took 0.573s\n",
      "  training loss:\t\t0.109831\n",
      "Epoch 102 of 200 took 0.597s\n",
      "  training loss:\t\t0.108912\n",
      "Epoch 103 of 200 took 0.594s\n",
      "  training loss:\t\t0.108478\n",
      "Epoch 104 of 200 took 0.608s\n",
      "  training loss:\t\t0.107697\n",
      "Epoch 105 of 200 took 0.602s\n",
      "  training loss:\t\t0.106831\n",
      "Epoch 106 of 200 took 0.605s\n",
      "  training loss:\t\t0.106868\n",
      "Epoch 107 of 200 took 0.574s\n",
      "  training loss:\t\t0.106285\n",
      "Epoch 108 of 200 took 0.573s\n",
      "  training loss:\t\t0.106604\n",
      "Epoch 109 of 200 took 0.590s\n",
      "  training loss:\t\t0.106076\n",
      "Epoch 110 of 200 took 0.575s\n",
      "  training loss:\t\t0.105505\n",
      "Epoch 111 of 200 took 0.572s\n",
      "  training loss:\t\t0.104733\n",
      "Epoch 112 of 200 took 0.574s\n",
      "  training loss:\t\t0.104506\n",
      "Epoch 113 of 200 took 0.583s\n",
      "  training loss:\t\t0.104529\n",
      "Epoch 114 of 200 took 0.602s\n",
      "  training loss:\t\t0.104220\n",
      "Epoch 115 of 200 took 0.585s\n",
      "  training loss:\t\t0.103526\n",
      "Epoch 116 of 200 took 0.587s\n",
      "  training loss:\t\t0.103638\n",
      "Epoch 117 of 200 took 0.580s\n",
      "  training loss:\t\t0.102992\n",
      "Epoch 118 of 200 took 0.573s\n",
      "  training loss:\t\t0.102488\n",
      "Epoch 119 of 200 took 0.575s\n",
      "  training loss:\t\t0.102917\n",
      "Epoch 120 of 200 took 0.589s\n",
      "  training loss:\t\t0.102503\n",
      "Epoch 121 of 200 took 0.590s\n",
      "  training loss:\t\t0.102158\n",
      "Epoch 122 of 200 took 0.583s\n",
      "  training loss:\t\t0.102070\n",
      "Epoch 123 of 200 took 0.580s\n",
      "  training loss:\t\t0.100961\n",
      "Epoch 124 of 200 took 0.578s\n",
      "  training loss:\t\t0.101106\n",
      "Epoch 125 of 200 took 0.586s\n",
      "  training loss:\t\t0.100295\n",
      "Epoch 126 of 200 took 0.616s\n",
      "  training loss:\t\t0.099832\n",
      "Epoch 127 of 200 took 0.624s\n",
      "  training loss:\t\t0.099838\n",
      "Epoch 128 of 200 took 0.597s\n",
      "  training loss:\t\t0.099367\n",
      "Epoch 129 of 200 took 0.612s\n",
      "  training loss:\t\t0.099522\n",
      "Epoch 130 of 200 took 0.597s\n",
      "  training loss:\t\t0.098788\n",
      "Epoch 131 of 200 took 0.620s\n",
      "  training loss:\t\t0.098593\n",
      "Epoch 132 of 200 took 0.608s\n",
      "  training loss:\t\t0.098864\n",
      "Epoch 133 of 200 took 0.615s\n",
      "  training loss:\t\t0.098289\n",
      "Epoch 134 of 200 took 0.626s\n",
      "  training loss:\t\t0.097702\n",
      "Epoch 135 of 200 took 0.613s\n",
      "  training loss:\t\t0.098303\n",
      "Epoch 136 of 200 took 0.624s\n",
      "  training loss:\t\t0.097669\n",
      "Epoch 137 of 200 took 0.615s\n",
      "  training loss:\t\t0.097391\n",
      "Epoch 138 of 200 took 0.614s\n",
      "  training loss:\t\t0.096396\n",
      "Epoch 139 of 200 took 0.592s\n",
      "  training loss:\t\t0.096639\n",
      "Epoch 140 of 200 took 0.646s\n",
      "  training loss:\t\t0.096183\n",
      "Epoch 141 of 200 took 0.597s\n",
      "  training loss:\t\t0.096112\n",
      "Epoch 142 of 200 took 0.584s\n",
      "  training loss:\t\t0.096081\n",
      "Epoch 143 of 200 took 0.605s\n",
      "  training loss:\t\t0.095620\n",
      "Epoch 144 of 200 took 0.582s\n",
      "  training loss:\t\t0.095286\n",
      "Epoch 145 of 200 took 0.593s\n",
      "  training loss:\t\t0.095192\n",
      "Epoch 146 of 200 took 0.575s\n",
      "  training loss:\t\t0.094791\n",
      "Epoch 147 of 200 took 0.626s\n",
      "  training loss:\t\t0.094458\n",
      "Epoch 148 of 200 took 0.709s\n",
      "  training loss:\t\t0.094123\n",
      "Epoch 149 of 200 took 0.585s\n",
      "  training loss:\t\t0.094551\n",
      "Epoch 150 of 200 took 0.598s\n",
      "  training loss:\t\t0.094128\n",
      "Epoch 151 of 200 took 0.642s\n",
      "  training loss:\t\t0.093569\n",
      "Epoch 152 of 200 took 0.590s\n",
      "  training loss:\t\t0.093870\n",
      "Epoch 153 of 200 took 0.577s\n",
      "  training loss:\t\t0.093617\n",
      "Epoch 154 of 200 took 0.689s\n",
      "  training loss:\t\t0.093379\n",
      "Epoch 155 of 200 took 0.628s\n",
      "  training loss:\t\t0.092728\n",
      "Epoch 156 of 200 took 0.647s\n",
      "  training loss:\t\t0.092074\n",
      "Epoch 157 of 200 took 0.635s\n",
      "  training loss:\t\t0.092538\n",
      "Epoch 158 of 200 took 0.641s\n",
      "  training loss:\t\t0.092047\n",
      "Epoch 159 of 200 took 0.624s\n",
      "  training loss:\t\t0.092346\n",
      "Epoch 160 of 200 took 0.626s\n",
      "  training loss:\t\t0.091865\n",
      "Epoch 161 of 200 took 0.618s\n",
      "  training loss:\t\t0.091684\n",
      "Epoch 162 of 200 took 0.618s\n",
      "  training loss:\t\t0.091555\n",
      "Epoch 163 of 200 took 0.633s\n",
      "  training loss:\t\t0.091541\n",
      "Epoch 164 of 200 took 0.636s\n",
      "  training loss:\t\t0.090800\n",
      "Epoch 165 of 200 took 0.640s\n",
      "  training loss:\t\t0.090781\n",
      "Epoch 166 of 200 took 0.638s\n",
      "  training loss:\t\t0.090563\n",
      "Epoch 167 of 200 took 0.622s\n",
      "  training loss:\t\t0.091173\n",
      "Epoch 168 of 200 took 0.622s\n",
      "  training loss:\t\t0.090391\n",
      "Epoch 169 of 200 took 0.626s\n",
      "  training loss:\t\t0.090153\n",
      "Epoch 170 of 200 took 0.628s\n",
      "  training loss:\t\t0.089959\n",
      "Epoch 171 of 200 took 0.623s\n",
      "  training loss:\t\t0.089612\n",
      "Epoch 172 of 200 took 0.622s\n",
      "  training loss:\t\t0.089141\n",
      "Epoch 173 of 200 took 0.610s\n",
      "  training loss:\t\t0.089157\n",
      "Epoch 174 of 200 took 0.614s\n",
      "  training loss:\t\t0.089082\n",
      "Epoch 175 of 200 took 0.619s\n",
      "  training loss:\t\t0.089112\n",
      "Epoch 176 of 200 took 0.612s\n",
      "  training loss:\t\t0.088682\n",
      "Epoch 177 of 200 took 0.613s\n",
      "  training loss:\t\t0.088821\n",
      "Epoch 178 of 200 took 0.606s\n",
      "  training loss:\t\t0.088262\n",
      "Epoch 179 of 200 took 0.612s\n",
      "  training loss:\t\t0.088142\n",
      "Epoch 180 of 200 took 0.618s\n",
      "  training loss:\t\t0.088556\n",
      "Epoch 181 of 200 took 0.606s\n",
      "  training loss:\t\t0.087960\n",
      "Epoch 182 of 200 took 0.605s\n",
      "  training loss:\t\t0.088001\n",
      "Epoch 183 of 200 took 0.612s\n",
      "  training loss:\t\t0.087495\n",
      "Epoch 184 of 200 took 0.608s\n",
      "  training loss:\t\t0.086956\n",
      "Epoch 185 of 200 took 0.602s\n",
      "  training loss:\t\t0.086745\n",
      "Epoch 186 of 200 took 0.600s\n",
      "  training loss:\t\t0.086770\n",
      "Epoch 187 of 200 took 0.602s\n",
      "  training loss:\t\t0.087226\n",
      "Epoch 188 of 200 took 0.604s\n",
      "  training loss:\t\t0.086743\n",
      "Epoch 189 of 200 took 0.602s\n",
      "  training loss:\t\t0.086211\n",
      "Epoch 190 of 200 took 0.602s\n",
      "  training loss:\t\t0.086351\n",
      "Epoch 191 of 200 took 0.622s\n",
      "  training loss:\t\t0.086172\n",
      "Epoch 192 of 200 took 0.631s\n",
      "  training loss:\t\t0.086454\n",
      "Epoch 193 of 200 took 0.608s\n",
      "  training loss:\t\t0.085846\n",
      "Epoch 194 of 200 took 0.607s\n",
      "  training loss:\t\t0.085944\n",
      "Epoch 195 of 200 took 0.604s\n",
      "  training loss:\t\t0.085767\n",
      "Epoch 196 of 200 took 0.602s\n",
      "  training loss:\t\t0.085249\n",
      "Epoch 197 of 200 took 0.603s\n",
      "  training loss:\t\t0.085322\n",
      "Epoch 198 of 200 took 0.606s\n",
      "  training loss:\t\t0.085306\n",
      "Epoch 199 of 200 took 0.613s\n",
      "  training loss:\t\t0.085006\n",
      "Epoch 200 of 200 took 0.602s\n",
      "  training loss:\t\t0.084401\n",
      "Final results:\n",
      "  test loss:\t\t\t0.141112\n",
      "  test accuracy:\t\t96.74 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "input_var = T.matrix('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "network = build_NIN_model(input_var, 1024, 128)\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "l2_penalty = regularize_network_params(network, l2)\n",
    "loss = loss.mean() + 1e-4*l2_penalty\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.adagrad(loss, params, learning_rate=0.05)\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                    dtype=theano.config.floatX)\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "        # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "final_acc = test_acc / test_batches\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    final_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
